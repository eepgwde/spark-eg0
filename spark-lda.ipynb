{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spylon Kernel Test with Spark 3.4.0\n",
    "\n",
    "This has been updated from Spark 2.4. I use a local SBT installation via /misc/build/0/classes. This is similar to the PySpark spark0 notebook.\n",
    "\n",
    "This must use the same Scala version as Spark - which is 2.13 (it was 2.11).\n",
    "\n",
    "I haven't recompiled the Scala source code in src - the artikus.spark classes.\n",
    "\n",
    "Once a Spark context is instantiated, it should be accessible from http://j1:4040 if the host of this notebook is j1. This hostname is spark.driver.host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Of no use for a Spylon notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Initialization of Spark\n",
    "\n",
    "Note that we can set things like driver memory etc.\n",
    "\n",
    "If `launcher._spark_home` is not set it will default to looking at the `SPARK_HOME` environment variable.\n",
    "\n",
    "I run on a cluster owned by the hadoop user who is a member of my group devel.\n",
    "\n",
    "I build new features for Scala and access them via /misc/build/0/classes. I have to restart the kernel to access any new classes. And must relaunch Spark to access changes.\n",
    "\n",
    "I can't change the spark.sql.warehouse.dir\n",
    "\n",
    "This loads external JARs - com.johnsnowlabs.nlp - and its dependencies with ivy. These go to .ivy2/cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master = \"yarn\"\n",
    "launcher.conf.spark.app.name = \"spark-lda\"\n",
    "launcher.conf.spark.executor.cores = 8\n",
    "launcher.num_executors = 4\n",
    "launcher.executor_cores = 4\n",
    "launcher.driver_memory = '4g'\n",
    "launcher.conf.set(\"spark.driver.cores\", 4);\n",
    "launcher.conf.set(\"spark.executor.cores\", 4);\n",
    "launcher.conf.set(\"spark.executor.memory\", \"4g\");\n",
    "launcher.conf.set(\"spark.executor.instances\", 4);\n",
    "launcher.conf.set(\"spark.sql.warehouse.dir\", \"file:///home/hadoop/data/hive\");\n",
    "launcher.conf.set(\"spark.sql.catalogImplementation\", \"hive\");\n",
    "launcher.conf.set(\"spark.hadoop.fs.permissions.umask-mode\", \"002\");\n",
    "launcher.conf.set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.2\");\n",
    "launcher.conf.set(\"spark.driver.extraClassPath\", \":/misc/build/0/classes/:/usr/share/java/postgresql.jar\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Spark Configuration\n",
    "\n",
    "Some basic operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://k1:8088/proxy/application_1684145519388_0001\n",
       "SparkContext available as 'sc' (version = 3.4.0, master = yarn, app id = application_1684145519388_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@228596c2\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark // spark is the SQL session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: String = 3.4.0\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.warehouse.dir --> file:/home/hadoop/data/hive\n",
      "spark.hadoop.fs.permissions.umask-mode --> 002\n",
      "spark.executor.extraJavaOptions --> -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.host --> j1.host0\n",
      "spark.serializer.objectStreamReset --> 100\n",
      "spark.driver.port --> 42869\n",
      "spark.rdd.compress --> True\n",
      "spark.repl.class.uri --> spark://j1.host0:42869/classes\n",
      "spark.repl.class.outputDir --> /var/tmp/tmpbyipmdnf\n",
      "spark.app.name --> spylon-kernel\n",
      "spark.driver.memory --> 4g\n",
      "spark.executor.instances --> 4\n",
      "spark.submit.pyFiles --> \n",
      "spark.ui.showConsoleProgress --> true\n",
      "spark.app.submitTime --> 1684141882895\n",
      "spark.app.startTime --> 1684141883393\n",
      "spark.executor.id --> driver\n",
      "spark.driver.cores --> 4\n",
      "spark.driver.extraJavaOptions --> -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.submit.deployMode --> client\n",
      "spark.master --> local[*]\n",
      "spark.executor.memory --> 4g\n",
      "spark.driver.extraClassPath --> :/misc/build/0/classes/:/usr/share/java/postgresql.jar\n",
      "spark.sql.catalogImplementation --> hive\n",
      "spark.executor.cores --> 4\n",
      "spark.app.id --> local-1684141886448\n"
     ]
    }
   ],
   "source": [
    "spark.conf.getAll foreach (x => println(x._1 + \" --> \" + x._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------------------+--------------------+\n",
      "|   name|      catalog|         description|         locationUri|\n",
      "+-------+-------------+--------------------+--------------------+\n",
      "|default|spark_catalog|Default Hive data...|file:/misc/build/...|\n",
      "+-------+-------------+--------------------+--------------------+\n",
      "\n",
      "+-------------+-----------+\n",
      "|         name|description|\n",
      "+-------------+-----------+\n",
      "|spark_catalog|       null|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dbs1: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Database] = [name: string, catalog: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dbs1 = spark.catalog.listDatabases()\n",
    "dbs1.show\n",
    "spark.catalog.listCatalogs().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d0: Array[org.apache.spark.sql.catalog.Database] = Array(Database[name='default', catalog='spark_catalog', description='Default Hive database', path='file:/misc/build/0/spark-eg0/spark-warehouse'])\n",
       "res4: String = file:/misc/build/0/spark-eg0/spark-warehouse\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d0 = spark.catalog.listDatabases().take(1)\n",
    "d0(0).locationUri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|  default|finaltable|      false|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df0: org.apache.spark.sql.DataFrame = [count(1): bigint]\n",
       "df0: org.apache.spark.sql.DataFrame = [count(1): bigint]\n",
       "df0: org.apache.spark.sql.DataFrame = [count(1): bigint]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df0 = spark.sql(\"show databases\")\n",
    "df0.show()\n",
    "df0 = spark.sql(\"show tables\")\n",
    "df0.show()\n",
    "df0 = spark.sql(\"select count(*) from finalTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Using local Scala Builds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://j1.host0:4040\n",
       "SparkContext available as 'sc' (version = 3.4.0, master = local[*], app id = local-1684091628830)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import artikus.spark.U\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import artikus.spark.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.ClassCastException",
     "evalue": " class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')",
     "output_type": "error",
     "traceback": [
      "java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "val cl = spark.getClass().getClassLoader()\n",
    "cl.asInstanceOf[java.net.URLClassLoader].getURLs.map(x => x.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.sql.SparkSession\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "// These are from the /misc/build/0/classes\n",
    "U.identity\n",
    "U.printClass(spark)\n",
    "U.alert(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.classes(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.flist(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SparkSession operations\n",
    "\n",
    "Basic operations\n",
    "https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkSession.html#createDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val strings = spark.emptyDataset[String]\n",
    "strings.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val one = spark.createDataset(Seq(1))\n",
    "one.show\n",
    "one.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use an implicit requires a \"spark\" in the namespace.\n",
    "import spark.implicits._\n",
    "\n",
    "val one = Seq(1).toDS\n",
    "one.show\n",
    "one.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Using spark.range()\n",
    "val range0 = spark.range(start = 0, end = 4, step = 2, numPartitions = 5)\n",
    "range0.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// More packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = Seq(\"a\", \"b\", \"c\", \"d\") zip (0 to 4)\n",
    "\n",
    "U.printClass(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = Seq(\"foo\", \"bar\", \"baz\") zip 1 :: 2 :: 3 :: Nil\n",
    "val data1 = Seq(\"foo\", \"bar\", \"bar\") zip 4 :: 5 :: 6 :: Nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds = spark.createDataset(data)\n",
    "\n",
    "val ds1 = sc.parallelize(data)\n",
    "\n",
    "U.printClass(ds)\n",
    "U.printClass(ds1)\n",
    "\n",
    "val ds2 = sc.parallelize(data1)\n",
    "\n",
    "ds1.join(ds2).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Local file URI\n",
    "// non-existent file loads\n",
    "// /misc/build/0/prog-scala-2nd-ed-code-examples\n",
    "val local2 = U.local1(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val f1 = \"rev-users.csv\"\n",
    "val file = sc.textFile(local2(f1).toString())\n",
    "U.printClass(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This file has a header row\n",
    "// Take the first row, index into it, split and return a sequence\n",
    "val h2 = file.take(1)(0).split(\",\").toSeq\n",
    "\n",
    "// Get the remainder by using subtract\n",
    "// convert the header row back to an RDD using parallelize\n",
    "val r1 = file.subtract(sc.parallelize(file.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Look at the underlying row\n",
    "r1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now map over the quantities\n",
    "// The transformations are only applied when we take(), use the column names from h2.\n",
    "val df0 = r1.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b.toInt,c,d,e,f.toInt,g,h,i,j.toInt,k.toInt,l.toInt)}.toDF(h2:_*)\n",
    "df0.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val f2 = \"rev-devices.csv\"\n",
    "val file2 = sc.textFile(local2(f2).toString())\n",
    "U.printClass(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// But error results here if file does not exist\n",
    "// Or returns empty array if it is empty\n",
    "val lens = file.map(s => s.length)\n",
    "file.take(5)\n",
    "lens.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val x0 = file.take(1)\n",
    "\n",
    "// Some arbitrary file processing - append a number to each line\n",
    "val pairs = file.map(s => (s, 911))\n",
    "val counts = pairs.reduceByKey((a, b) => a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val counts1 = counts.repartition(1)\n",
    "\n",
    "U.rmdir(\"counts1\")\n",
    "counts1.saveAsTextFile(local2(\"counts1\").toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pairs = file.map(x => (x.split(\",\")(0), x))\n",
    "\n",
    "val pairs1 = pairs.join(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Make some (K, V) tuples\n",
    "\n",
    "println(x0(0))\n",
    "\n",
    "val x1 = x0(0).split(\",\").toSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df0 = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b,c,d,e,f,g,h,i,j,k,l)}.toDF(x1:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The x1:_* is to be preferred to this\n",
    "\n",
    "// val fileToDf = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "// (a,b,c,d,e,f,g,h,i,j,k,l)}.toDF(\"user_id\", \"birth_year\", \"country\", \"city\", \"created_date\", \"user_settings_crypto_unlocked\", \"plan\", \"attributes_notifications_marketing_push\", \"attributes_notifications_marketing_email\", \"num_contacts\", \"num_referrals\", \"num_successful_referrals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df0 = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b.toInt,c,d,e,f,g,h,i,j,k,l)}.toDF(x1:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileToDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.map(_.split(\",\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df1 = file.subtract(sc.parallelize(file.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.printClass(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(f1:String, sep:String)(implicit sc: org.apache.spark.SparkContext) : org.apache.spark.rdd.RDD[String] = {\n",
    "    val f = sc.textFile(f1)\n",
    "    return f\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split(local2(f1).toString(), \",\")(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.printClass(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLLib\n",
    "\n",
    "Using LDA from this example. The data chosen is not very good. The headlines don't have enough words to trigger on.\n",
    "\n",
    "https://medium.com/analytics-vidhya/distributed-topic-modelling-using-spark-nlp-and-spark-mllib-lda-6db3f06a4da3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://k1:8088/proxy/application_1684147981528_0001\n",
       "SparkContext available as 'sc' (version = 3.4.0, master = yarn, app id = application_1684147981528_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "url: String = file:///a/l/X-image/cache/data/abcnews-date-text.csv\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val url = \"file:///a/l/X-image/cache/data/abcnews-date-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type0: String = csv\n",
       "infer_schema: String = true\n",
       "first_row_is_header: String = true\n",
       "delimiter: String = ,\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val type0=\"csv\"\n",
    "val infer_schema=\"true\"\n",
    "val first_row_is_header = \"true\"\n",
    "val delimiter=\",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df0: org.apache.spark.sql.DataFrame = [publish_date: int, headline_text: string]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df0 = spark.read.format(type0)\n",
    "  .option(\"inferSchema\", infer_schema)\n",
    "  .option(\"header\", first_row_is_header)\n",
    "  .option(\"sep\", delimiter)\n",
    "  .load(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Long = 1082168\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import com.johnsnowlabs.nlp.DocumentAssembler\n",
       "import com.johnsnowlabs.nlp.annotators.Tokenizer\n",
       "import com.johnsnowlabs.nlp.annotators.Normalizer\n",
       "import com.johnsnowlabs.nlp.annotators.StopWordsCleaner\n",
       "import com.johnsnowlabs.nlp.annotators.Stemmer\n",
       "import com.johnsnowlabs.nlp.Finisher\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.johnsnowlabs.nlp.DocumentAssembler\n",
    "import com.johnsnowlabs.nlp.annotators.Tokenizer\n",
    "import com.johnsnowlabs.nlp.annotators.Normalizer\n",
    "import com.johnsnowlabs.nlp.annotators.StopWordsCleaner\n",
    "import com.johnsnowlabs.nlp.annotators.Stemmer\n",
    "import com.johnsnowlabs.nlp.Finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document_assembler: com.johnsnowlabs.nlp.DocumentAssembler = document_c879f8ee263e\n",
       "tokenizer: com.johnsnowlabs.nlp.annotators.Tokenizer = REGEX_TOKENIZER_84c9b7813d48\n",
       "normalizer: com.johnsnowlabs.nlp.annotators.Normalizer = NORMALIZER_37d0d62bb66f\n",
       "stopwords_cleaner: com.johnsnowlabs.nlp.annotators.StopWordsCleaner = STOPWORDS_CLEANER_e6ad3685cf9e\n",
       "stemmer: com.johnsnowlabs.nlp.annotators.Stemmer = STEMMER_075cdcf04187\n",
       "finisher: com.johnsnowlabs.nlp.Finisher = finisher_02ed352a6956\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Split sentence to tokens(array)\n",
    "val document_assembler = new DocumentAssembler().setInputCol(\"headline_text\").setOutputCol(\"document\").setCleanupMode(\"shrink\") \n",
    "\n",
    "// clean unwanted characters and garbage\n",
    "val tokenizer = new Tokenizer().setInputCols(Array(\"document\")).setOutputCol(\"token\")\n",
    "\n",
    "val normalizer = new Normalizer().setInputCols(Array(\"token\")).setOutputCol(\"normalized\")\n",
    "\n",
    "// remove stopwords\n",
    "val stopwords_cleaner = new StopWordsCleaner().setInputCols(\"normalized\").setOutputCol(\"cleanTokens\").setCaseSensitive(false)\n",
    "\n",
    "// stem the words to bring them to the root form.\n",
    "val stemmer = new Stemmer().setInputCols(Array(\"cleanTokens\")).setOutputCol(\"stem\")\n",
    "\n",
    "// Finisher is the most important annotator. \n",
    "// Spark NLP adds its own structure when we convert each row in the dataframe to document. \n",
    "// Finisher helps us to bring back the expected structure viz. array of tokens.\n",
    "val finisher = new Finisher().setInputCols(Array(\"stem\")).setOutputCols(Array(\"tokens\"))\n",
    "    .setOutputAsArray(true).setCleanAnnotations(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable] = Array(document_c879f8ee263e, REGEX_TOKENIZER_84c9b7813d48, NORMALIZER_37d0d62bb66f, STOPWORDS_CLEANER_e6ad3685cf9e, STEMMER_075cdcf04187, finisher_02ed352a6956)\n",
       "nlp_pipeline: org.apache.spark.ml.Pipeline = pipeline_80f149cb34b5\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We build a ml pipeline so that each phase can be executed in sequence. \n",
    "// This pipeline can also be used to test the model. \n",
    "// train the pipeline\n",
    "val stages=Array(document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, finisher)\n",
    "val nlp_pipeline = new Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlp_model: org.apache.spark.ml.PipelineModel = pipeline_80f149cb34b5\n",
       "processed_df0: org.apache.spark.sql.DataFrame = [publish_date: int, headline_text: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//  apply the pipeline to transform dataframe.\n",
    "val nlp_model = nlp_pipeline.fit(df0) \n",
    "val processed_df0  = nlp_model.transform(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|publish_date|              tokens|\n",
      "+------------+--------------------+\n",
      "|    20030219|[aba, decid, comm...|\n",
      "|    20030219|[act, fire, wit, ...|\n",
      "|    20030219|[g, call, infrast...|\n",
      "|    20030219|[air, nz, staff, ...|\n",
      "|    20030219|[air, nz, strike,...|\n",
      "|    20030219|[ambiti, olsson, ...|\n",
      "|    20030219|[antic, delight, ...|\n",
      "|    20030219|[aussi, qualifi, ...|\n",
      "|    20030219|[aust, address, u...|\n",
      "|    20030219|[australia, lock,...|\n",
      "|    20030219|[australia, contr...|\n",
      "|    20030219|[barca, take, rec...|\n",
      "|    20030219|[bathhous, plan, ...|\n",
      "|    20030219|[big, hope, launc...|\n",
      "|    20030219|[big, plan, boost...|\n",
      "|    20030219|[blizzard, buri, ...|\n",
      "|    20030219|[brigadi, dismiss...|\n",
      "|    20030219|[british, combat,...|\n",
      "|    20030219|[bryant, lead, la...|\n",
      "|    20030219|[bushfir, victim,...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokens_df0: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [publish_date: int, tokens: array<string>]\n"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//  nlp pipeline create intermediary columns that we dont need. So lets select the columns that we need\n",
    "val tokens_df0 = processed_df0.select(\"publish_date\",\"tokens\").limit(100000)\n",
    "tokens_df0.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "To generate features from textual data. Latent Dirichlet Allocation requires a data-specific vocabulary to perform topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.CountVectorizer\n"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.feature.CountVectorizer = cntVec_97c8dbd96b5b\n",
       "cv_model: org.apache.spark.ml.feature.CountVectorizerModel = CountVectorizerModel: uid=cntVec_97c8dbd96b5b, vocabularySize=500\n",
       "vectorized_tokens: org.apache.spark.sql.DataFrame = [publish_date: int, tokens: array<string> ... 1 more field]\n"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cv = new CountVectorizer().setInputCol(\"tokens\").setOutputCol(\"features\").setVocabSize(500).setMinTF(3.0)\n",
    "\n",
    "// train the model\n",
    "val cv_model = cv.fit(tokens_df0)\n",
    "// transform the data. Output column name will be features.\n",
    "val vectorized_tokens = cv_model.transform(tokens_df0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.clustering.LDA\n",
       "import org.apache.spark.sql.Row\n"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.clustering.LDA\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_topics: Int = 5\n",
       "lda: org.apache.spark.ml.clustering.LDA = lda_8cc6a6e4ad3e\n",
       "model: org.apache.spark.ml.clustering.LDAModel = LocalLDAModel: uid=lda_8cc6a6e4ad3e, k=5, numFeatures=500\n",
       "ll: Double = -991.6942161586913\n",
       "lp: Double = 110.18824623985459\n"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val num_topics = 5\n",
    "val lda = new LDA().setK(num_topics).setMaxIter(100)\n",
    "val model = lda.fit(vectorized_tokens)\n",
    "\n",
    "val ll = model.logLikelihood(vectorized_tokens)\n",
    "val lp = model.logPerplexity(vectorized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -991.6942161586913\n",
      "The upper bound on perplexity: 110.18824623985459\n"
     ]
    }
   ],
   "source": [
    "println(\"The lower bound on the log likelihood of the entire corpus: \" + ll.toString())\n",
    "println(\"The upper bound on perplexity: \" + lp.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices              |termWeights                                                                                                       |\n",
      "+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[300, 69, 359, 493, 286] |[0.002655236612160945, 0.002497961414830468, 0.0024899769599476575, 0.0024601846493057153, 0.00244648449557187]   |\n",
      "|1    |[239, 369, 201, 329, 98] |[0.0025134790115381414, 0.0025013211527450227, 0.0024899204926820107, 0.0024621807786108577, 0.002460169764610191]|\n",
      "|2    |[319, 298, 177, 155, 401]|[0.015238049286679918, 0.0025746153634851575, 0.002442532964731303, 0.0024237449892337945, 0.0024162537292844602] |\n",
      "|3    |[261, 491, 145, 248, 316]|[0.00253436396446723, 0.0025207354462662523, 0.0024758172318584096, 0.0024419313969263026, 0.002416577924771319]  |\n",
      "|4    |[384, 54, 348, 89, 242]  |[0.0502144102029174, 0.023875924207515532, 0.00244757199004047, 0.002424430034488546, 0.002387678985252054]       |\n",
      "+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topics: org.apache.spark.sql.DataFrame = [topic: int, termIndices: array<int> ... 1 more field]\n"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topics = model.describeTopics(num_topics)\n",
    "println(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------------------------+-----------+---------------------+\n",
      "|publish_date|tokens                                             |features   |topicDistribution    |\n",
      "+------------+---------------------------------------------------+-----------+---------------------+\n",
      "|20030219    |[aba, decid, commun, broadcast, licenc]            |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[act, fire, wit, must, awar, defam]                |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[g, call, infrastructur, protect, summit]          |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[air, nz, staff, aust, strike, pai, rise]          |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[air, nz, strike, affect, australian, travel]      |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[ambiti, olsson, win, tripl, jump]                 |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[antic, delight, record, break, barca]             |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[aussi, qualifi, stosur, wast, four, memphi, match]|(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[aust, address, un, secur, council, iraq]          |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[australia, lock, war, timet, opp]                 |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[australia, contribut, million, aid, iraq]         |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[barca, take, record, robson, celebr, birthdai]    |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[bathhous, plan, move, ahead]                      |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[big, hope, launceston, cycl, championship]        |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[big, plan, boost, paroo, water, suppli]           |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[blizzard, buri, unit, state, bill]                |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[brigadi, dismiss, report, troop, harass]          |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[british, combat, troop, arriv, daili, kuwait]     |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[bryant, lead, laker, doubl, overtim, win]         |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[bushfir, victim, urg, see, centrelink]            |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "+------------+---------------------------------------------------+-----------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformed: org.apache.spark.sql.DataFrame = [publish_date: int, tokens: array<string> ... 2 more fields]\n"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transformed = model.transform(vectorized_tokens)\n",
    "transformed.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res164: Array[org.apache.spark.sql.Row] = Array([WrappedArray(aba, decid, commun, broadcast, licenc)], [WrappedArray(act, fire, wit, must, awar, defam)], [WrappedArray(g, call, infrastructur, protect, summit)], [WrappedArray(air, nz, staff, aust, strike, pai, rise)], [WrappedArray(air, nz, strike, affect, australian, travel)], [WrappedArray(ambiti, olsson, win, tripl, jump)], [WrappedArray(antic, delight, record, break, barca)], [WrappedArray(aussi, qualifi, stosur, wast, four, memphi, match)], [WrappedArray(aust, address, un, secur, council, iraq)], [WrappedArray(australia, lock, war, timet, opp)], [WrappedArray(australia, contribut, million, aid, iraq)], [WrappedArray(barca, take, record, robson, celebr, birthdai)], [WrappedArray(bathhous, plan, move, ahead)], [WrappedArray(big, hope,...\n"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_tokens.select(\"tokens\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "The results from the algorithm need to restructured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vocab: Array[String] = Array(u, polic, govt, new, plan, man, council, iraq, sai, call, kill, win, charg, court, back, face, claim, urg, fund, report, warn, fire, nsw, boost, take, get, attack, set, death, water, qld, wa, mai, probe, group, consid, seek, crash, hospit, open, continu, world, health, concern, cup, miss, lead, hope, mp, pm, protest, meet, help, servic, test, two, murder, chang, hit, iraqi, australia, minist, ban, talk, drug, war, sydnei, home, vic, secur, sa, rise, support, year, top, industri, road, bomb, work, investig, final, nt, air, car, offer, welcom, return, fear, act, reject, jail, case, hous, forc, job, fight, record, make, strike, end, deal, defend, worker, arrest, pai, trial, australian, power, public, farmer, m, cut, move, rule, still, dai, dead, south, want, tr...\n"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocab = cv_model.vocabulary\n",
    "val topics = model.describeTopics()\n",
    "val topics_rdd = topics.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false),StructField(indices,ArrayType(IntegerType,true),true),StructField(scores,ArrayType(DoubleType,true),true))\n"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define the schema and make a data frame with it\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val schema = new StructType()\n",
    "  .add(StructField(\"id\", IntegerType, false))\n",
    "  .add(StructField(\"indices\", ArrayType(IntegerType, true)))\n",
    "  .add(StructField(\"scores\", ArrayType(DoubleType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- indices: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- scores: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "df1: org.apache.spark.sql.DataFrame = [id: int, indices: array<int> ... 1 more field]\n"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "// import org.apache.spark.sql.functions.explode\n",
    "\n",
    "val df1 = spark.createDataFrame(topics_rdd, schema)\n",
    "\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class tab1\n"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Using the column names create a case class that has the Arrays in it\n",
    "case class tab1(id: Int, indices: WrappedArray[Int], scores: WrappedArray[Double])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|             indices|              scores|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[300, 69, 359, 49...|[0.00273718541331...|\n",
      "|  1|[239, 369, 201, 3...|[0.00257811336156...|\n",
      "|  2|[298, 177, 155, 4...|[0.00268410608851...|\n",
      "|  3|[261, 491, 145, 2...|[0.00260131780330...|\n",
      "|  4|[54, 384, 348, 89...|[0.00956404662531...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.Dataset[tab1] = [id: int, indices: array<int> ... 1 more field]\n"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cast the dataframe to be of that type.\n",
    "val df2 = df1.as[tab1]\n",
    "df2.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3: List[scala.collection.mutable.WrappedArray[(String, Double)]] = List(WrappedArray((poll,0.0027371854133140853), (secur,0.002560246657611293), (raid,0.002551263934741385), (line,0.0025177467974942472), (bill,0.002502333762281136), (teacher,0.0024809462044122427), (race,0.002456433101087516), (act,0.0024250307641168), (nt,0.0024169064692088882), (author,0.002416614667940068)), WrappedArray((korea,0.002578113361563147), (nurs,0.002564425666431991), (doctor,0.002551590449154687), (increas,0.002520360217278145), (strike,0.00251809615524107), (poll,0.0025062644280818887), (british,0.0025029408660668896), (kill,0.002490891680355644), (brisban,0.0024788558521510163), (tip,0.002477245307779051)), WrappedArray((storm,0.0026841060885122938), (look,0.002533607378645535), (tour,0.00251219978686...\n"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Then the columns can be accessed as members and with types.\n",
    "// this uses the indices into the vocab \n",
    "\n",
    "val df3 = df2.map(x => x.indices.map(vocab).zip(x.scores)).collect.toList.map {\n",
    "    _.map(x => (x._1, x._2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::\n",
      "poll :: 0.0027371854133140853\n",
      "secur :: 0.002560246657611293\n",
      "raid :: 0.002551263934741385\n",
      "line :: 0.0025177467974942472\n",
      "bill :: 0.002502333762281136\n",
      "teacher :: 0.0024809462044122427\n",
      "race :: 0.002456433101087516\n",
      "act :: 0.0024250307641168\n",
      "nt :: 0.0024169064692088882\n",
      "author :: 0.002416614667940068\n",
      "::\n",
      "korea :: 0.002578113361563147\n",
      "nurs :: 0.002564425666431991\n",
      "doctor :: 0.002551590449154687\n",
      "increas :: 0.002520360217278145\n",
      "strike :: 0.00251809615524107\n",
      "poll :: 0.0025062644280818887\n",
      "british :: 0.0025029408660668896\n",
      "kill :: 0.002490891680355644\n",
      "brisban :: 0.0024788558521510163\n",
      "tip :: 0.002477245307779051\n",
      "::\n",
      "storm :: 0.0026841060885122938\n",
      "look :: 0.002533607378645535\n",
      "tour :: 0.0025121997868690366\n",
      "women :: 0.002503664017014095\n",
      "remain :: 0.002479214785423427\n",
      "top :: 0.0024574859047018613\n",
      "question :: 0.002451903299087713\n",
      "rais :: 0.0024474415348164436\n",
      "g :: 0.0024332839219606646\n",
      "busi :: 0.0024330620637832406\n",
      "::\n",
      "offic :: 0.002601317803300338\n",
      "post :: 0.0025859822436945655\n",
      "deni :: 0.0025354377938667898\n",
      "teacher :: 0.0024973075853049124\n",
      "rais :: 0.00246877846146971\n",
      "youth :: 0.002451744412811615\n",
      "bu :: 0.002450555161823485\n",
      "safeti :: 0.002417046425760324\n",
      "line :: 0.0024123531929994\n",
      "children :: 0.0024107722975154422\n",
      "::\n",
      "test :: 0.00956404662531811\n",
      "near :: 0.006006883514845379\n",
      "hill :: 0.0026802569212485768\n",
      "reject :: 0.0026525859055762705\n",
      "injuri :: 0.0026086423910744445\n",
      "wast :: 0.002577756770694025\n",
      "number :: 0.0025020135527626976\n",
      "woman :: 0.002488459380385237\n",
      "melbourn :: 0.0024613136917275937\n",
      "close :: 0.0024547922758129865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res149: List[scala.collection.mutable.WrappedArray[Unit]] = List(WrappedArray((), (), (), (), (), (), (), (), (), ()), WrappedArray((), (), (), (), (), (), (), (), (), ()), WrappedArray((), (), (), (), (), (), (), (), (), ()), WrappedArray((), (), (), (), (), (), (), (), (), ()), WrappedArray((), (), (), (), (), (), (), (), (), ()))\n"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.map(y => { println(\"::\"); y.map(x => println(x._1 + \" :: \" + x._2) ) } );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|  default|finaltable|      false|\n",
      "|         |    topics|       true|\n",
      "+---------+----------+-----------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show\n",
    "spark.sql(\"select count(*) from topics\").show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel~"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
