{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spylon Kernel Test with Spark 3.4.0\n",
    "\n",
    "This has been updated from Spark 2.4. I use a local SBT installation via /misc/build/0/classes. This is similar to the PySpark spark0 notebook.\n",
    "\n",
    "This must use the same Scala version as Spark - which is 2.13 (it was 2.11).\n",
    "\n",
    "I haven't recompiled the Scala source code in src - the artikus.spark classes.\n",
    "\n",
    "Once a Spark context is instantiated, it should be accessible from http://j1:4040 if the host of this notebook is j1. This hostname is spark.driver.host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Of no use for a Spylon notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Initialization of Spark\n",
    "\n",
    "Note that we can set things like driver memory etc.\n",
    "\n",
    "If `launcher._spark_home` is not set it will default to looking at the `SPARK_HOME` environment variable.\n",
    "\n",
    "I run on a cluster owned by the hadoop user who is a member of my group devel.\n",
    "\n",
    "I build new features for Scala and access them via /misc/build/0/classes. I have to restart the kernel to access any new classes. And must relaunch Spark to access changes.\n",
    "\n",
    "I can't change the spark.sql.warehouse.dir. There is no explicit command to enableHiveSupport and no configuration.\n",
    "\n",
    "This loads external JARs - com.johnsnowlabs.nlp - and its dependencies with ivy. These go to .ivy2/cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master = \"local[*]\"\n",
    "launcher.conf.spark.app.name = \"spark-lda\"\n",
    "launcher.conf.spark.executor.cores = 8\n",
    "launcher.num_executors = 4\n",
    "launcher.executor_cores = 4\n",
    "launcher.driver_memory = '4g'\n",
    "launcher.conf.set(\"spark.driver.cores\", 4);\n",
    "launcher.conf.set(\"spark.executor.cores\", 4);\n",
    "launcher.conf.set(\"spark.executor.memory\", \"4g\");\n",
    "launcher.conf.set(\"spark.executor.instances\", 4);\n",
    "launcher.conf.set(\"spark.sql.catalogImplementation\", \"hive\");\n",
    "launcher.conf.set(\"spark.sql.warehouse.dir\", \"file:///home/hadoop/data/hive\");\n",
    "launcher.conf.set(\"spark.hadoop.fs.permissions.umask-mode\", \"002\");\n",
    "launcher.conf.set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.2\");\n",
    "launcher.conf.set(\"spark.driver.extraClassPath\", \":/misc/build/0/classes/:/usr/share/java/postgresql.jar\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark Configuration\n",
    "\n",
    "Some basic operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://j1.host0:4040\n",
       "SparkContext available as 'sc' (version = 3.4.0, master = local[*], app id = local-1684620375479)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{DataFrame, Encoder, Encoders, Row, SparkSession}\n",
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@58c6f79b\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{DataFrame, Encoder, Encoders, Row, SparkSession}\n",
    "SparkSession.builder().enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@58c6f79b\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: String = 3.4.0\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.warehouse.dir --> file:/home/hadoop/data/hive\n",
      "spark.hadoop.fs.permissions.umask-mode --> 002\n",
      "spark.executor.extraJavaOptions --> -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.host --> j1.host0\n",
      "spark.serializer.objectStreamReset --> 100\n",
      "spark.driver.port --> 44353\n",
      "spark.rdd.compress --> True\n",
      "spark.repl.class.uri --> spark://j1.host0:44353/classes\n",
      "spark.jars --> file:///cache/weaves/2/ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.2.jar,file:///cache/weaves/2/ivy2/jars/com.typesafe_config-1.4.2.jar,file:///cache/weaves/2/ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar,file:///cache/weaves/2/ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar,file:///cache/weaves/2/ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-storage-2.16.0.jar,file:///cache/weaves/2/ivy2/jars/com.navigamez_greex-1.0.jar,file:///cache/weaves/2/ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar,file:///cache/weaves/2/ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,file:///cache/weaves/2/ivy2/jars/org.projectlombok_lombok-1.16.8.jar,file:///cache/weaves/2/ivy2/jars/com.google.guava_guava-31.1-jre.jar,file:///cache/weaves/2/ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,file:///cache/weaves/2/ivy2/jars/com.google.errorprone_error_prone_annotations-2.16.jar,file:///cache/weaves/2/ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-gson-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.api-client_google-api-client-2.1.1.jar,file:///cache/weaves/2/ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///cache/weaves/2/ivy2/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.code.gson_gson-2.10.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-httpjson-0.105.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-core-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-2.20.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-grpc-2.20.1.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-alts-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-grpclb-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-protobuf-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_api-common-2.2.2.jar,file:///cache/weaves/2/ivy2/jars/javax.annotation_javax.annotation-api-1.3.2.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-api-0.31.1.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-context-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar,file:///cache/weaves/2/ivy2/jars/com.google.protobuf_protobuf-java-3.21.10.jar,file:///cache/weaves/2/ivy2/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar,file:///cache/weaves/2/ivy2/jars/org.threeten_threetenbp-1.6.4.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-api-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-auth-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-stub-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/org.checkerframework_checker-qual-3.28.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.android_annotations-4.1.1.4.jar,file:///cache/weaves/2/ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-netty-shaded-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.perfmark_perfmark-api-0.26.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-googleapis-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-xds-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-proto-0.2.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-services-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.re2j_re2j-1.6.jar,file:///cache/weaves/2/ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar\n",
      "spark.repl.class.outputDir --> /var/tmp/tmp87uhvrey\n",
      "spark.app.name --> spark-lda\n",
      "spark.app.initial.file.urls --> file:///cache/weaves/2/ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,file:///cache/weaves/2/ivy2/jars/com.typesafe_config-1.4.2.jar,file:///cache/weaves/2/ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,file:///cache/weaves/2/ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.code.gson_gson-2.10.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-netty-shaded-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-storage-2.16.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar,file:///cache/weaves/2/ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar,file:///cache/weaves/2/ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar,file:///cache/weaves/2/ivy2/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-api-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-protobuf-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-googleapis-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-xds-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-2.20.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-gson-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-context-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.api-client_google-api-client-2.1.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///cache/weaves/2/ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,file:///cache/weaves/2/ivy2/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-alts-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/org.checkerframework_checker-qual-3.28.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.errorprone_error_prone_annotations-2.16.jar,file:///cache/weaves/2/ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,file:///cache/weaves/2/ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar,file:///cache/weaves/2/ivy2/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.protobuf_protobuf-java-3.21.10.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-httpjson-0.105.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_api-common-2.2.2.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-api-0.31.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-grpc-2.20.1.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-proto-0.2.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/javax.annotation_javax.annotation-api-1.3.2.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-grpclb-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-auth-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.perfmark_perfmark-api-0.26.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-services-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-stub-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.android_annotations-4.1.1.4.jar,file:///cache/weaves/2/ivy2/jars/org.projectlombok_lombok-1.16.8.jar,file:///cache/weaves/2/ivy2/jars/com.google.re2j_re2j-1.6.jar,file:///cache/weaves/2/ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,file:///cache/weaves/2/ivy2/jars/com.navigamez_greex-1.0.jar,file:///cache/weaves/2/ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.2.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-core-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/org.threeten_threetenbp-1.6.4.jar,file:///cache/weaves/2/ivy2/jars/com.google.guava_guava-31.1-jre.jar\n",
      "spark.driver.memory --> 4g\n",
      "spark.executor.instances --> 4\n",
      "spark.submit.pyFiles --> /cache/weaves/2/ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.2.jar,/cache/weaves/2/ivy2/jars/com.typesafe_config-1.4.2.jar,/cache/weaves/2/ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar,/cache/weaves/2/ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar,/cache/weaves/2/ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,/cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-storage-2.16.0.jar,/cache/weaves/2/ivy2/jars/com.navigamez_greex-1.0.jar,/cache/weaves/2/ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar,/cache/weaves/2/ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,/cache/weaves/2/ivy2/jars/org.projectlombok_lombok-1.16.8.jar,/cache/weaves/2/ivy2/jars/com.google.guava_guava-31.1-jre.jar,/cache/weaves/2/ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,/cache/weaves/2/ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,/cache/weaves/2/ivy2/jars/com.google.errorprone_error_prone_annotations-2.16.jar,/cache/weaves/2/ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,/cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-1.42.3.jar,/cache/weaves/2/ivy2/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar,/cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar,/cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-gson-1.42.3.jar,/cache/weaves/2/ivy2/jars/com.google.api-client_google-api-client-2.1.1.jar,/cache/weaves/2/ivy2/jars/commons-codec_commons-codec-1.15.jar,/cache/weaves/2/ivy2/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar,/cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar,/cache/weaves/2/ivy2/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar,/cache/weaves/2/ivy2/jars/com.google.code.gson_gson-2.10.jar,/cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-2.9.0.jar,/cache/weaves/2/ivy2/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar,/cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar,/cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar,/cache/weaves/2/ivy2/jars/com.google.api_gax-httpjson-0.105.1.jar,/cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-core-1.51.0.jar,/cache/weaves/2/ivy2/jars/com.google.api_gax-2.20.1.jar,/cache/weaves/2/ivy2/jars/com.google.api_gax-grpc-2.20.1.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-alts-1.51.0.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-grpclb-1.51.0.jar,/cache/weaves/2/ivy2/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-protobuf-1.51.0.jar,/cache/weaves/2/ivy2/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar,/cache/weaves/2/ivy2/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar,/cache/weaves/2/ivy2/jars/com.google.api_api-common-2.2.2.jar,/cache/weaves/2/ivy2/jars/javax.annotation_javax.annotation-api-1.3.2.jar,/cache/weaves/2/ivy2/jars/io.opencensus_opencensus-api-0.31.1.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-context-1.51.0.jar,/cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar,/cache/weaves/2/ivy2/jars/com.google.protobuf_protobuf-java-3.21.10.jar,/cache/weaves/2/ivy2/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar,/cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar,/cache/weaves/2/ivy2/jars/org.threeten_threetenbp-1.6.4.jar,/cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar,/cache/weaves/2/ivy2/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar,/cache/weaves/2/ivy2/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar,/cache/weaves/2/ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar,/cache/weaves/2/ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-api-1.51.0.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-auth-1.51.0.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-stub-1.51.0.jar,/cache/weaves/2/ivy2/jars/org.checkerframework_checker-qual-3.28.0.jar,/cache/weaves/2/ivy2/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar,/cache/weaves/2/ivy2/jars/com.google.android_annotations-4.1.1.4.jar,/cache/weaves/2/ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-netty-shaded-1.51.0.jar,/cache/weaves/2/ivy2/jars/io.perfmark_perfmark-api-0.26.0.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-googleapis-1.51.0.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-xds-1.51.0.jar,/cache/weaves/2/ivy2/jars/io.opencensus_opencensus-proto-0.2.0.jar,/cache/weaves/2/ivy2/jars/io.grpc_grpc-services-1.51.0.jar,/cache/weaves/2/ivy2/jars/com.google.re2j_re2j-1.6.jar,/cache/weaves/2/ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar\n",
      "spark.ui.showConsoleProgress --> true\n",
      "spark.app.submitTime --> 1684620363816\n",
      "spark.jars.packages --> com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.2\n",
      "spark.files --> file:///cache/weaves/2/ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.2.jar,file:///cache/weaves/2/ivy2/jars/com.typesafe_config-1.4.2.jar,file:///cache/weaves/2/ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar,file:///cache/weaves/2/ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar,file:///cache/weaves/2/ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-storage-2.16.0.jar,file:///cache/weaves/2/ivy2/jars/com.navigamez_greex-1.0.jar,file:///cache/weaves/2/ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar,file:///cache/weaves/2/ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,file:///cache/weaves/2/ivy2/jars/org.projectlombok_lombok-1.16.8.jar,file:///cache/weaves/2/ivy2/jars/com.google.guava_guava-31.1-jre.jar,file:///cache/weaves/2/ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,file:///cache/weaves/2/ivy2/jars/com.google.errorprone_error_prone_annotations-2.16.jar,file:///cache/weaves/2/ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-gson-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.api-client_google-api-client-2.1.1.jar,file:///cache/weaves/2/ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///cache/weaves/2/ivy2/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.code.gson_gson-2.10.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-httpjson-0.105.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-core-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-2.20.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-grpc-2.20.1.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-alts-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-grpclb-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-protobuf-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_api-common-2.2.2.jar,file:///cache/weaves/2/ivy2/jars/javax.annotation_javax.annotation-api-1.3.2.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-api-0.31.1.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-context-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar,file:///cache/weaves/2/ivy2/jars/com.google.protobuf_protobuf-java-3.21.10.jar,file:///cache/weaves/2/ivy2/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar,file:///cache/weaves/2/ivy2/jars/org.threeten_threetenbp-1.6.4.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-api-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-auth-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-stub-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/org.checkerframework_checker-qual-3.28.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.android_annotations-4.1.1.4.jar,file:///cache/weaves/2/ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-netty-shaded-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.perfmark_perfmark-api-0.26.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-googleapis-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-xds-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-proto-0.2.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-services-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.re2j_re2j-1.6.jar,file:///cache/weaves/2/ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar\n",
      "spark.app.startTime --> 1684620364857\n",
      "spark.executor.id --> driver\n",
      "spark.driver.cores --> 4\n",
      "spark.driver.extraJavaOptions --> -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.app.initial.jar.urls --> spark://j1.host0:44353/jars/com.google.android_annotations-4.1.1.4.jar,spark://j1.host0:44353/jars/com.google.cloud_google-cloud-core-2.9.0.jar,spark://j1.host0:44353/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar,spark://j1.host0:44353/jars/com.google.http-client_google-http-client-gson-1.42.3.jar,spark://j1.host0:44353/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar,spark://j1.host0:44353/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.2.jar,spark://j1.host0:44353/jars/io.opencensus_opencensus-proto-0.2.0.jar,spark://j1.host0:44353/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar,spark://j1.host0:44353/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar,spark://j1.host0:44353/jars/org.rocksdb_rocksdbjni-6.29.5.jar,spark://j1.host0:44353/jars/io.grpc_grpc-netty-shaded-1.51.0.jar,spark://j1.host0:44353/jars/io.opencensus_opencensus-api-0.31.1.jar,spark://j1.host0:44353/jars/com.google.guava_failureaccess-1.0.1.jar,spark://j1.host0:44353/jars/com.google.api_api-common-2.2.2.jar,spark://j1.host0:44353/jars/com.typesafe_config-1.4.2.jar,spark://j1.host0:44353/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar,spark://j1.host0:44353/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar,spark://j1.host0:44353/jars/com.google.errorprone_error_prone_annotations-2.16.jar,spark://j1.host0:44353/jars/io.grpc_grpc-grpclb-1.51.0.jar,spark://j1.host0:44353/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar,spark://j1.host0:44353/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,spark://j1.host0:44353/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar,spark://j1.host0:44353/jars/javax.annotation_javax.annotation-api-1.3.2.jar,spark://j1.host0:44353/jars/io.grpc_grpc-api-1.51.0.jar,spark://j1.host0:44353/jars/io.grpc_grpc-auth-1.51.0.jar,spark://j1.host0:44353/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar,spark://j1.host0:44353/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar,spark://j1.host0:44353/jars/it.unimi.dsi_fastutil-7.0.12.jar,spark://j1.host0:44353/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar,spark://j1.host0:44353/jars/com.google.code.gson_gson-2.10.jar,spark://j1.host0:44353/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar,spark://j1.host0:44353/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar,spark://j1.host0:44353/jars/org.threeten_threetenbp-1.6.4.jar,spark://j1.host0:44353/jars/com.navigamez_greex-1.0.jar,spark://j1.host0:44353/jars/com.google.api_gax-httpjson-0.105.1.jar,spark://j1.host0:44353/jars/io.grpc_grpc-alts-1.51.0.jar,spark://j1.host0:44353/jars/commons-codec_commons-codec-1.15.jar,spark://j1.host0:44353/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar,spark://j1.host0:44353/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar,spark://j1.host0:44353/jars/io.perfmark_perfmark-api-0.26.0.jar,spark://j1.host0:44353/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,spark://j1.host0:44353/jars/io.grpc_grpc-core-1.51.0.jar,spark://j1.host0:44353/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar,spark://j1.host0:44353/jars/io.grpc_grpc-googleapis-1.51.0.jar,spark://j1.host0:44353/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar,spark://j1.host0:44353/jars/com.google.api_gax-2.20.1.jar,spark://j1.host0:44353/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar,spark://j1.host0:44353/jars/com.google.cloud_google-cloud-storage-2.16.0.jar,spark://j1.host0:44353/jars/com.google.api-client_google-api-client-2.1.1.jar,spark://j1.host0:44353/jars/com.google.j2objc_j2objc-annotations-1.3.jar,spark://j1.host0:44353/jars/io.grpc_grpc-context-1.51.0.jar,spark://j1.host0:44353/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar,spark://j1.host0:44353/jars/com.google.re2j_re2j-1.6.jar,spark://j1.host0:44353/jars/io.grpc_grpc-stub-1.51.0.jar,spark://j1.host0:44353/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar,spark://j1.host0:44353/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar,spark://j1.host0:44353/jars/io.grpc_grpc-xds-1.51.0.jar,spark://j1.host0:44353/jars/org.projectlombok_lombok-1.16.8.jar,spark://j1.host0:44353/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar,spark://j1.host0:44353/jars/io.grpc_grpc-protobuf-1.51.0.jar,spark://j1.host0:44353/jars/com.google.api_gax-grpc-2.20.1.jar,spark://j1.host0:44353/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar,spark://j1.host0:44353/jars/io.grpc_grpc-services-1.51.0.jar,spark://j1.host0:44353/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar,spark://j1.host0:44353/jars/com.google.guava_guava-31.1-jre.jar,spark://j1.host0:44353/jars/org.checkerframework_checker-qual-3.28.0.jar,spark://j1.host0:44353/jars/com.google.http-client_google-http-client-1.42.3.jar,spark://j1.host0:44353/jars/dk.brics.automaton_automaton-1.11-8.jar,spark://j1.host0:44353/jars/com.google.protobuf_protobuf-java-3.21.10.jar,spark://j1.host0:44353/jars/com.google.code.findbugs_jsr305-3.0.2.jar\n",
      "spark.submit.deployMode --> client\n",
      "spark.master --> local[*]\n",
      "spark.executor.memory --> 4g\n",
      "spark.driver.extraClassPath --> :/misc/build/0/classes/:/usr/share/java/postgresql.jar\n",
      "spark.sql.catalogImplementation --> hive\n",
      "spark.executor.cores --> 4\n",
      "spark.repl.local.jars --> file:///cache/weaves/2/ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.2.jar,file:///cache/weaves/2/ivy2/jars/com.typesafe_config-1.4.2.jar,file:///cache/weaves/2/ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar,file:///cache/weaves/2/ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar,file:///cache/weaves/2/ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-storage-2.16.0.jar,file:///cache/weaves/2/ivy2/jars/com.navigamez_greex-1.0.jar,file:///cache/weaves/2/ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar,file:///cache/weaves/2/ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar,file:///cache/weaves/2/ivy2/jars/org.projectlombok_lombok-1.16.8.jar,file:///cache/weaves/2/ivy2/jars/com.google.guava_guava-31.1-jre.jar,file:///cache/weaves/2/ivy2/jars/com.google.guava_failureaccess-1.0.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar,file:///cache/weaves/2/ivy2/jars/com.google.errorprone_error_prone_annotations-2.16.jar,file:///cache/weaves/2/ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-gson-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.api-client_google-api-client-2.1.1.jar,file:///cache/weaves/2/ivy2/jars/commons-codec_commons-codec-1.15.jar,file:///cache/weaves/2/ivy2/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.code.gson_gson-2.10.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-httpjson-0.105.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-core-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-2.20.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_gax-grpc-2.20.1.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-alts-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-grpclb-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-protobuf-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api_api-common-2.2.2.jar,file:///cache/weaves/2/ivy2/jars/javax.annotation_javax.annotation-api-1.3.2.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-api-0.31.1.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-context-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar,file:///cache/weaves/2/ivy2/jars/com.google.protobuf_protobuf-java-3.21.10.jar,file:///cache/weaves/2/ivy2/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar,file:///cache/weaves/2/ivy2/jars/org.threeten_threetenbp-1.6.4.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar,file:///cache/weaves/2/ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar,file:///cache/weaves/2/ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-api-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-auth-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-stub-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/org.checkerframework_checker-qual-3.28.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.android_annotations-4.1.1.4.jar,file:///cache/weaves/2/ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-netty-shaded-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.perfmark_perfmark-api-0.26.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-googleapis-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-xds-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/io.opencensus_opencensus-proto-0.2.0.jar,file:///cache/weaves/2/ivy2/jars/io.grpc_grpc-services-1.51.0.jar,file:///cache/weaves/2/ivy2/jars/com.google.re2j_re2j-1.6.jar,file:///cache/weaves/2/ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar\n",
      "spark.app.id --> local-1684620375479\n"
     ]
    }
   ],
   "source": [
    "spark.conf.getAll foreach (x => println(x._1 + \" --> \" + x._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------------------+--------------------+\n",
      "|   name|      catalog|         description|         locationUri|\n",
      "+-------+-------------+--------------------+--------------------+\n",
      "|default|spark_catalog|Default Hive data...|hdfs://k1:8020/us...|\n",
      "+-------+-------------+--------------------+--------------------+\n",
      "\n",
      "+-------------+-----------+\n",
      "|         name|description|\n",
      "+-------------+-----------+\n",
      "|spark_catalog|       null|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dbs1: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Database] = [name: string, catalog: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dbs1 = spark.catalog.listDatabases()\n",
    "dbs1.show\n",
    "spark.catalog.listCatalogs().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: org.apache.spark.sql.catalog.Database = Database[name='default', catalog='spark_catalog', description='Default Hive database', path='hdfs://k1:8020/user/hive/warehouse']\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbs1.take(1)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+----------+-----------+\n",
      "|namespace|tableName |isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|default  |finaltable|false      |\n",
      "|default  |stage0    |false      |\n",
      "|default  |stage1    |false      |\n",
      "|default  |xusers    |false      |\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df0: org.apache.spark.sql.DataFrame = [count(1): bigint]\n",
       "df0: org.apache.spark.sql.DataFrame = [count(1): bigint]\n",
       "df0: org.apache.spark.sql.DataFrame = [count(1): bigint]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df0 = spark.sql(\"show databases\")\n",
    "df0.show()\n",
    "df0 = spark.sql(\"show tables\")\n",
    "df0.show(truncate=false)\n",
    "df0 = spark.sql(\"select count(*) from finalTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stage1: org.apache.spark.sql.DataFrame = [publish_date: int, tokens: array<string> ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stage1 = spark.sql(\"select * from stage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables().select(\"name\").collect().map(_.getString(0)).filter(_.contains(\"_\"))\n",
    " .foreach(x => spark.sql(s\"drop table ${x}\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------------------------+---------------------------+\n",
      "|publish_date|tokens                                             |cntvec_79841e49b2ea__output|\n",
      "+------------+---------------------------------------------------+---------------------------+\n",
      "|20030219    |[aba, decid, commun, broadcast, licenc]            |{0, 500, [], []}           |\n",
      "|20030219    |[act, fire, wit, must, awar, defam]                |{0, 500, [], []}           |\n",
      "|20030219    |[g, call, infrastructur, protect, summit]          |{0, 500, [], []}           |\n",
      "|20030219    |[air, nz, staff, aust, strike, pai, rise]          |{0, 500, [], []}           |\n",
      "|20030219    |[air, nz, strike, affect, australian, travel]      |{0, 500, [], []}           |\n",
      "|20030219    |[ambiti, olsson, win, tripl, jump]                 |{0, 500, [], []}           |\n",
      "|20030219    |[antic, delight, record, break, barca]             |{0, 500, [], []}           |\n",
      "|20030219    |[aussi, qualifi, stosur, wast, four, memphi, match]|{0, 500, [], []}           |\n",
      "|20030219    |[aust, address, un, secur, council, iraq]          |{0, 500, [], []}           |\n",
      "|20030219    |[australia, lock, war, timet, opp]                 |{0, 500, [], []}           |\n",
      "|20030219    |[australia, contribut, million, aid, iraq]         |{0, 500, [], []}           |\n",
      "|20030219    |[barca, take, record, robson, celebr, birthdai]    |{0, 500, [], []}           |\n",
      "|20030219    |[bathhous, plan, move, ahead]                      |{0, 500, [], []}           |\n",
      "|20030219    |[big, hope, launceston, cycl, championship]        |{0, 500, [], []}           |\n",
      "|20030219    |[big, plan, boost, paroo, water, suppli]           |{0, 500, [], []}           |\n",
      "|20030219    |[blizzard, buri, unit, state, bill]                |{0, 500, [], []}           |\n",
      "|20030219    |[brigadi, dismiss, report, troop, harass]          |{0, 500, [], []}           |\n",
      "|20030219    |[british, combat, troop, arriv, daili, kuwait]     |{0, 500, [], []}           |\n",
      "|20030219    |[bryant, lead, laker, doubl, overtim, win]         |{0, 500, [], []}           |\n",
      "|20030219    |[bushfir, victim, urg, see, centrelink]            |{0, 500, [], []}           |\n",
      "+------------+---------------------------------------------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df0: org.apache.spark.sql.DataFrame = [publish_date: int, tokens: array<string> ... 1 more field]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = spark.sql(\"select * from stage1\")\n",
    "df0.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res37: Long = 1\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.filter(size($\"cntvec_79841e49b2ea__output.indices\") !== 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- publish_date: integer (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- cntvec_79841e49b2ea__output: struct (nullable = true)\n",
      " |    |-- type: byte (nullable = true)\n",
      " |    |-- size: integer (nullable = true)\n",
      " |    |-- indices: array (nullable = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- values: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `features`.`indices` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`default`.`stage1`.`tokens`, `spark_catalog`.`default`.`stage1`.`publish_date`, `spark_catalog`.`default`.`stage1`.`cntvec_79841e49b2ea__output`].;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `features`.`indices` cannot be resolved. Did you mean one of the following? [`spark_catalog`.`default`.`stage1`.`tokens`, `spark_catalog`.`default`.`stage1`.`publish_date`, `spark_catalog`.`default`.`stage1`.`cntvec_79841e49b2ea__output`].;",
      "'Project ['features.indices]",
      "+- Project [publish_date#97, tokens#98, cntvec_79841e49b2ea__output#99]",
      "   +- SubqueryAlias spark_catalog.default.stage1",
      "      +- HiveTableRelation [`spark_catalog`.`default`.`stage1`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [publish_date#97, tokens#98, cntvec_79841e49b2ea__output#99], Partition Cols: []]",
      "",
      "  at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:221)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:143)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:258)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:256)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4(CheckAnalysis.scala:256)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$4$adapted(CheckAnalysis.scala:256)",
      "  at scala.collection.immutable.Stream.foreach(Stream.scala:533)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:256)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)",
      "  at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4196)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1578)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1595)",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "stage1.select(\"features.indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res38: Int = 0\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage1.filter(size(col(\"features.indices\")) > 1).collect().size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using local Scala Builds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://j1.host0:4040\n",
       "SparkContext available as 'sc' (version = 3.4.0, master = local[*], app id = local-1684091628830)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import artikus.spark.U\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import artikus.spark.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.ClassCastException",
     "evalue": " class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')",
     "output_type": "error",
     "traceback": [
      "java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "val cl = spark.getClass().getClassLoader()\n",
    "cl.asInstanceOf[java.net.URLClassLoader].getURLs.map(x => x.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.sql.SparkSession\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "// These are from the /misc/build/0/classes\n",
    "U.identity\n",
    "U.printClass(spark)\n",
    "U.alert(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.classes(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.flist(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SparkSession operations\n",
    "\n",
    "Basic operations\n",
    "https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkSession.html#createDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val strings = spark.emptyDataset[String]\n",
    "strings.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val one = spark.createDataset(Seq(1))\n",
    "one.show\n",
    "one.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use an implicit requires a \"spark\" in the namespace.\n",
    "import spark.implicits._\n",
    "\n",
    "val one = Seq(1).toDS\n",
    "one.show\n",
    "one.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Using spark.range()\n",
    "val range0 = spark.range(start = 0, end = 4, step = 2, numPartitions = 5)\n",
    "range0.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// More packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = Seq(\"a\", \"b\", \"c\", \"d\") zip (0 to 4)\n",
    "\n",
    "U.printClass(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = Seq(\"foo\", \"bar\", \"baz\") zip 1 :: 2 :: 3 :: Nil\n",
    "val data1 = Seq(\"foo\", \"bar\", \"bar\") zip 4 :: 5 :: 6 :: Nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds = spark.createDataset(data)\n",
    "\n",
    "val ds1 = sc.parallelize(data)\n",
    "\n",
    "U.printClass(ds)\n",
    "U.printClass(ds1)\n",
    "\n",
    "val ds2 = sc.parallelize(data1)\n",
    "\n",
    "ds1.join(ds2).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Local file URI\n",
    "// non-existent file loads\n",
    "// /misc/build/0/prog-scala-2nd-ed-code-examples\n",
    "val local2 = U.local1(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val f1 = \"rev-users.csv\"\n",
    "val file = sc.textFile(local2(f1).toString())\n",
    "U.printClass(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This file has a header row\n",
    "// Take the first row, index into it, split and return a sequence\n",
    "val h2 = file.take(1)(0).split(\",\").toSeq\n",
    "\n",
    "// Get the remainder by using subtract\n",
    "// convert the header row back to an RDD using parallelize\n",
    "val r1 = file.subtract(sc.parallelize(file.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Look at the underlying row\n",
    "r1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now map over the quantities\n",
    "// The transformations are only applied when we take(), use the column names from h2.\n",
    "val df0 = r1.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b.toInt,c,d,e,f.toInt,g,h,i,j.toInt,k.toInt,l.toInt)}.toDF(h2:_*)\n",
    "df0.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val f2 = \"rev-devices.csv\"\n",
    "val file2 = sc.textFile(local2(f2).toString())\n",
    "U.printClass(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// But error results here if file does not exist\n",
    "// Or returns empty array if it is empty\n",
    "val lens = file.map(s => s.length)\n",
    "file.take(5)\n",
    "lens.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val x0 = file.take(1)\n",
    "\n",
    "// Some arbitrary file processing - append a number to each line\n",
    "val pairs = file.map(s => (s, 911))\n",
    "val counts = pairs.reduceByKey((a, b) => a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val counts1 = counts.repartition(1)\n",
    "\n",
    "U.rmdir(\"counts1\")\n",
    "counts1.saveAsTextFile(local2(\"counts1\").toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pairs = file.map(x => (x.split(\",\")(0), x))\n",
    "\n",
    "val pairs1 = pairs.join(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Make some (K, V) tuples\n",
    "\n",
    "println(x0(0))\n",
    "\n",
    "val x1 = x0(0).split(\",\").toSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df0 = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b,c,d,e,f,g,h,i,j,k,l)}.toDF(x1:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The x1:_* is to be preferred to this\n",
    "\n",
    "// val fileToDf = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "// (a,b,c,d,e,f,g,h,i,j,k,l)}.toDF(\"user_id\", \"birth_year\", \"country\", \"city\", \"created_date\", \"user_settings_crypto_unlocked\", \"plan\", \"attributes_notifications_marketing_push\", \"attributes_notifications_marketing_email\", \"num_contacts\", \"num_referrals\", \"num_successful_referrals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df0 = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b.toInt,c,d,e,f,g,h,i,j,k,l)}.toDF(x1:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileToDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.map(_.split(\",\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df1 = file.subtract(sc.parallelize(file.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.printClass(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(f1:String, sep:String)(implicit sc: org.apache.spark.SparkContext) : org.apache.spark.rdd.RDD[String] = {\n",
    "    val f = sc.textFile(f1)\n",
    "    return f\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split(local2(f1).toString(), \",\")(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.printClass(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLLib\n",
    "\n",
    "Using LDA from this example. The data chosen is not very good. The headlines don't have enough words to trigger on.\n",
    "\n",
    "https://medium.com/analytics-vidhya/distributed-topic-modelling-using-spark-nlp-and-spark-mllib-lda-6db3f06a4da3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url: String = file:///a/l/X-image/cache/data/abcnews-date-text.csv\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val url = \"file:///a/l/X-image/cache/data/abcnews-date-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type0: String = csv\n",
       "infer_schema: String = true\n",
       "first_row_is_header: String = true\n",
       "delimiter: String = ,\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val type0=\"csv\"\n",
    "val infer_schema=\"true\"\n",
    "val first_row_is_header = \"true\"\n",
    "val delimiter=\",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df0: org.apache.spark.sql.DataFrame = [publish_date: int, headline_text: string]\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df0 = spark.read.format(type0)\n",
    "  .option(\"inferSchema\", infer_schema)\n",
    "  .option(\"header\", first_row_is_header)\n",
    "  .option(\"sep\", delimiter)\n",
    "  .load(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|publish_date|       headline_text|\n",
      "+------------+--------------------+\n",
      "|    20030219|aba decides again...|\n",
      "|    20030219|act fire witnesse...|\n",
      "|    20030219|a g calls for inf...|\n",
      "|    20030219|air nz staff in a...|\n",
      "|    20030219|air nz strike to ...|\n",
      "|    20030219|ambitious olsson ...|\n",
      "|    20030219|antic delighted w...|\n",
      "|    20030219|aussie qualifier ...|\n",
      "|    20030219|aust addresses un...|\n",
      "|    20030219|australia is lock...|\n",
      "|    20030219|australia to cont...|\n",
      "|    20030219|barca take record...|\n",
      "|    20030219|bathhouse plans m...|\n",
      "|    20030219|big hopes for lau...|\n",
      "|    20030219|big plan to boost...|\n",
      "|    20030219|blizzard buries u...|\n",
      "|    20030219|brigadier dismiss...|\n",
      "|    20030219|british combat tr...|\n",
      "|    20030219|bryant leads lake...|\n",
      "|    20030219|bushfire victims ...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.count()\n",
    "df0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import com.johnsnowlabs.nlp.DocumentAssembler\n",
       "import com.johnsnowlabs.nlp.annotators.Tokenizer\n",
       "import com.johnsnowlabs.nlp.annotators.Normalizer\n",
       "import com.johnsnowlabs.nlp.annotators.StopWordsCleaner\n",
       "import com.johnsnowlabs.nlp.annotators.Stemmer\n",
       "import com.johnsnowlabs.nlp.Finisher\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.johnsnowlabs.nlp.DocumentAssembler\n",
    "import com.johnsnowlabs.nlp.annotators.Tokenizer\n",
    "import com.johnsnowlabs.nlp.annotators.Normalizer\n",
    "import com.johnsnowlabs.nlp.annotators.StopWordsCleaner\n",
    "import com.johnsnowlabs.nlp.annotators.Stemmer\n",
    "import com.johnsnowlabs.nlp.Finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document_assembler: com.johnsnowlabs.nlp.DocumentAssembler = document_30ec569ce6fc\n",
       "tokenizer: com.johnsnowlabs.nlp.annotators.Tokenizer = REGEX_TOKENIZER_1b1a4c767d8a\n",
       "normalizer: com.johnsnowlabs.nlp.annotators.Normalizer = NORMALIZER_a6cb81d75b4a\n",
       "stopwords_cleaner: com.johnsnowlabs.nlp.annotators.StopWordsCleaner = STOPWORDS_CLEANER_a8269ae19c1f\n",
       "stemmer: com.johnsnowlabs.nlp.annotators.Stemmer = STEMMER_c317dffc1e47\n",
       "finisher: com.johnsnowlabs.nlp.Finisher = finisher_42b56442e7b2\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Split sentence to tokens(array)\n",
    "val document_assembler = new DocumentAssembler().setInputCol(\"headline_text\").setOutputCol(\"document\").setCleanupMode(\"shrink\") \n",
    "\n",
    "// clean unwanted characters and garbage\n",
    "val tokenizer = new Tokenizer().setInputCols(Array(\"document\")).setOutputCol(\"token\")\n",
    "\n",
    "val normalizer = new Normalizer().setInputCols(Array(\"token\")).setOutputCol(\"normalized\")\n",
    "\n",
    "// remove stopwords\n",
    "val stopwords_cleaner = new StopWordsCleaner().setInputCols(\"normalized\").setOutputCol(\"cleanTokens\").setCaseSensitive(false)\n",
    "\n",
    "// stem the words to bring them to the root form.\n",
    "val stemmer = new Stemmer().setInputCols(Array(\"cleanTokens\")).setOutputCol(\"stem\")\n",
    "\n",
    "// Finisher is the most important annotator. \n",
    "// Spark NLP adds its own structure when we convert each row in the dataframe to document. \n",
    "// Finisher helps us to bring back the expected structure viz. array of tokens.\n",
    "val finisher = new Finisher().setInputCols(Array(\"stem\")).setOutputCols(Array(\"tokens\"))\n",
    "    .setOutputAsArray(true).setCleanAnnotations(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable] = Array(document_30ec569ce6fc, REGEX_TOKENIZER_1b1a4c767d8a, NORMALIZER_a6cb81d75b4a, STOPWORDS_CLEANER_a8269ae19c1f, STEMMER_c317dffc1e47, finisher_42b56442e7b2)\n",
       "nlp_pipeline: org.apache.spark.ml.Pipeline = pipeline_a5fbc1e948c4\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We build a ml pipeline so that each phase can be executed in sequence. \n",
    "// This pipeline can also be used to test the model. \n",
    "// train the pipeline\n",
    "val stages=Array(document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, finisher)\n",
    "val nlp_pipeline = new Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlp_model: org.apache.spark.ml.PipelineModel = pipeline_a5fbc1e948c4\n",
       "processed_df0: org.apache.spark.sql.DataFrame = [publish_date: int, headline_text: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//  apply the pipeline to transform dataframe.\n",
    "val nlp_model = nlp_pipeline.fit(df0) \n",
    "val processed_df0  = nlp_model.transform(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: Long = 1082168\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 40.0 failed 1 times, most recent failure: Lost task 0.0 in stage 40.0 (TID 54) (j1.host0 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (HasSimpleAnnotate$$Lambda$5311/0x0000000801ff7040: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>).",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 40.0 failed 1 times, most recent failure: Lost task 0.0 in stage 40.0 (TID 54) (j1.host0 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (HasSimpleAnnotate$$Lambda$5311/0x0000000801ff7040: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>).",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)",
      "Caused by: java.lang.ClassCastException: class com.johnsnowlabs.nlp.util.regex.RuleFactory cannot be cast to class com.johnsnowlabs.nlp.util.regex.RuleFactory (com.johnsnowlabs.nlp.util.regex.RuleFactory is in unnamed module of loader scala.reflect.internal.util.ScalaClassLoader$URLClassLoader @5884c570; com.johnsnowlabs.nlp.util.regex.RuleFactory is in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @4b9c411)",
      "\tat com.johnsnowlabs.nlp.annotators.TokenizerModel.$anonfun$tag$5(TokenizerModel.scala:342)",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)",
      "\tat com.johnsnowlabs.nlp.annotators.TokenizerModel.$anonfun$tag$2(TokenizerModel.scala:400)",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "\tat com.johnsnowlabs.nlp.annotators.TokenizerModel.tag(TokenizerModel.scala:290)",
      "\tat com.johnsnowlabs.nlp.annotators.TokenizerModel.annotate(TokenizerModel.scala:408)",
      "\tat com.johnsnowlabs.nlp.HasSimpleAnnotate.$anonfun$dfAnnotate$1(HasSimpleAnnotate.scala:46)",
      "\t... 18 more",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:809)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:768)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:777)",
      "  ... 37 elided",
      "Caused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (HasSimpleAnnotate$$Lambda$5311/0x0000000801ff7040: (array<array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>>) => array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>).",
      "  at org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)",
      "  at org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)",
      "  at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:139)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)",
      "  at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)",
      "  ... 1 more",
      "Caused by: java.lang.ClassCastException: class com.johnsnowlabs.nlp.util.regex.RuleFactory cannot be cast to class com.johnsnowlabs.nlp.util.regex.RuleFactory (com.johnsnowlabs.nlp.util.regex.RuleFactory is in unnamed module of loader scala.reflect.internal.util.ScalaClassLoader$URLClassLoader @5884c570; com.johnsnowlabs.nlp.util.regex.RuleFactory is in unnamed module of loader org.apache.spark.util.MutableURLClassLoader @4b9c411)",
      "  at com.johnsnowlabs.nlp.annotators.TokenizerModel.$anonfun$tag$5(TokenizerModel.scala:342)",
      "  at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)",
      "  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)",
      "  at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)",
      "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)",
      "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)",
      "  at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)",
      "  at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)",
      "  at scala.collection.AbstractIterator.to(Iterator.scala:1431)",
      "  at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)",
      "  at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)",
      "  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)",
      "  at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)",
      "  at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)",
      "  at scala.collection.AbstractIterator.toArray(Iterator.scala:1431)",
      "  at com.johnsnowlabs.nlp.annotators.TokenizerModel.$anonfun$tag$2(TokenizerModel.scala:400)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at com.johnsnowlabs.nlp.annotators.TokenizerModel.tag(TokenizerModel.scala:290)",
      "  at com.johnsnowlabs.nlp.annotators.TokenizerModel.annotate(TokenizerModel.scala:408)",
      "  at com.johnsnowlabs.nlp.HasSimpleAnnotate.$anonfun$dfAnnotate$1(HasSimpleAnnotate.scala:46)",
      "  ... 18 more",
      ""
     ]
    }
   ],
   "source": [
    "//  nlp pipeline create intermediary columns that we dont need. So lets select the columns that we need\n",
    "val tokens_df0 = processed_df0.select(\"publish_date\",\"tokens\").limit(100000)\n",
    "tokens_df0.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "To generate features from textual data. Latent Dirichlet Allocation requires a data-specific vocabulary to perform topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.CountVectorizer\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.feature.CountVectorizer = cntVec_3a8a98006498\n",
       "cv_model: org.apache.spark.ml.feature.CountVectorizerModel = CountVectorizerModel: uid=cntVec_3a8a98006498, vocabularySize=500\n",
       "vectorized_tokens: org.apache.spark.sql.DataFrame = [publish_date: int, tokens: array<string> ... 1 more field]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cv = new CountVectorizer().setInputCol(\"tokens\").setOutputCol(\"features\").setVocabSize(500).setMinTF(3.0)\n",
    "\n",
    "// train the model\n",
    "val cv_model = cv.fit(tokens_df0)\n",
    "// transform the data. Output column name will be features.\n",
    "val vectorized_tokens = cv_model.transform(tokens_df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+\n",
      "|publish_date|              tokens|   features|\n",
      "+------------+--------------------+-----------+\n",
      "|    20030219|[aba, decid, comm...|(500,[],[])|\n",
      "|    20030219|[act, fire, wit, ...|(500,[],[])|\n",
      "|    20030219|[g, call, infrast...|(500,[],[])|\n",
      "|    20030219|[air, nz, staff, ...|(500,[],[])|\n",
      "|    20030219|[air, nz, strike,...|(500,[],[])|\n",
      "|    20030219|[ambiti, olsson, ...|(500,[],[])|\n",
      "|    20030219|[antic, delight, ...|(500,[],[])|\n",
      "|    20030219|[aussi, qualifi, ...|(500,[],[])|\n",
      "|    20030219|[aust, address, u...|(500,[],[])|\n",
      "|    20030219|[australia, lock,...|(500,[],[])|\n",
      "|    20030219|[australia, contr...|(500,[],[])|\n",
      "|    20030219|[barca, take, rec...|(500,[],[])|\n",
      "|    20030219|[bathhous, plan, ...|(500,[],[])|\n",
      "|    20030219|[big, hope, launc...|(500,[],[])|\n",
      "|    20030219|[big, plan, boost...|(500,[],[])|\n",
      "|    20030219|[blizzard, buri, ...|(500,[],[])|\n",
      "|    20030219|[brigadi, dismiss...|(500,[],[])|\n",
      "|    20030219|[british, combat,...|(500,[],[])|\n",
      "|    20030219|[bryant, lead, la...|(500,[],[])|\n",
      "|    20030219|[bushfir, victim,...|(500,[],[])|\n",
      "+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized_tokens.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.clustering.LDA\n",
       "import org.apache.spark.sql.Row\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.clustering.LDA\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_topics: Int = 5\n",
       "lda: org.apache.spark.ml.clustering.LDA = lda_ebc947d82821\n",
       "model: org.apache.spark.ml.clustering.LDAModel = LocalLDAModel: uid=lda_ebc947d82821, k=5, numFeatures=500\n",
       "ll: Double = -991.6942161586913\n",
       "lp: Double = 110.18824623985459\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val num_topics = 5\n",
    "val lda = new LDA().setK(num_topics).setMaxIter(100)\n",
    "val model = lda.fit(vectorized_tokens)\n",
    "\n",
    "val ll = model.logLikelihood(vectorized_tokens)\n",
    "val lp = model.logPerplexity(vectorized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -991.6942161586913\n",
      "The upper bound on perplexity: 110.18824623985459\n"
     ]
    }
   ],
   "source": [
    "println(\"The lower bound on the log likelihood of the entire corpus: \" + ll.toString())\n",
    "println(\"The upper bound on perplexity: \" + lp.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topics described by their top-weighted terms:\n",
      "+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices              |termWeights                                                                                                       |\n",
      "+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[300, 69, 359, 493, 286] |[0.002655236612160945, 0.002497961414830468, 0.0024899769599476575, 0.0024601846493057153, 0.00244648449557187]   |\n",
      "|1    |[239, 369, 201, 329, 98] |[0.0025134790115381414, 0.0025013211527450227, 0.0024899204926820107, 0.0024621807786108577, 0.002460169764610191]|\n",
      "|2    |[319, 298, 177, 155, 401]|[0.015238049286679918, 0.0025746153634851575, 0.002442532964731303, 0.0024237449892337945, 0.0024162537292844602] |\n",
      "|3    |[261, 491, 145, 248, 316]|[0.00253436396446723, 0.0025207354462662523, 0.0024758172318584096, 0.0024419313969263026, 0.002416577924771319]  |\n",
      "|4    |[384, 54, 348, 89, 242]  |[0.0502144102029174, 0.023875924207515532, 0.00244757199004047, 0.002424430034488546, 0.002387678985252054]       |\n",
      "+-----+-------------------------+------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topics: org.apache.spark.sql.DataFrame = [topic: int, termIndices: array<int> ... 1 more field]\n"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topics = model.describeTopics(num_topics)\n",
    "println(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------------------------+-----------+---------------------+\n",
      "|publish_date|tokens                                             |features   |topicDistribution    |\n",
      "+------------+---------------------------------------------------+-----------+---------------------+\n",
      "|20030219    |[aba, decid, commun, broadcast, licenc]            |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[act, fire, wit, must, awar, defam]                |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[g, call, infrastructur, protect, summit]          |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[air, nz, staff, aust, strike, pai, rise]          |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[air, nz, strike, affect, australian, travel]      |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[ambiti, olsson, win, tripl, jump]                 |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[antic, delight, record, break, barca]             |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[aussi, qualifi, stosur, wast, four, memphi, match]|(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[aust, address, un, secur, council, iraq]          |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[australia, lock, war, timet, opp]                 |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[australia, contribut, million, aid, iraq]         |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[barca, take, record, robson, celebr, birthdai]    |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[bathhous, plan, move, ahead]                      |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[big, hope, launceston, cycl, championship]        |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[big, plan, boost, paroo, water, suppli]           |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[blizzard, buri, unit, state, bill]                |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[brigadi, dismiss, report, troop, harass]          |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[british, combat, troop, arriv, daili, kuwait]     |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[bryant, lead, laker, doubl, overtim, win]         |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "|20030219    |[bushfir, victim, urg, see, centrelink]            |(500,[],[])|[0.0,0.0,0.0,0.0,0.0]|\n",
      "+------------+---------------------------------------------------+-----------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformed: org.apache.spark.sql.DataFrame = [publish_date: int, tokens: array<string> ... 2 more fields]\n",
       "tr0: Array[org.apache.spark.sql.Row] = Array([[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0.0]], [[0.0,0.0,0.0,0.0,0....\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transformed = model.transform(vectorized_tokens)\n",
    "val tr0 = transformed.select(\"topicDistribution\").collect()\n",
    "transformed.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adSum: (ad: Array[Double])Double\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adSum(ad: Array[Double]) = {\n",
    "  var sum = 0.0\n",
    "  var i = 0\n",
    "  while (i<ad.length) { sum += ad(i); i += 1 }\n",
    "  sum\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "// do some casting to array\n",
    "val tr1 = tr0.map(_(0)).map(_.asInstanceOf[org.apache.spark.ml.linalg.DenseVector].toArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res57: Array[Double] = Array(1.0, 0.9999999999999998, 1.0)\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr1.map(x => adSum(x) ).filter(_ > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: Array[Double] = Array(0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 60.0, 61.0, 62.0, 63.0, 64.0, 65.0, 66.0, 67.0, 68.0, 69.0, 70.0, 71.0, 72.0, 73.0, 74.0, 75.0, 76.0, 77.0, 78.0, 79.0, 80.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 89.0, 90.0, 91.0, 92.0, 93.0, 94.0, 95.0, 96.0, 97.0, 98.0, 99.0)\n",
       "ab: scala.collection.mutable.ArrayBuffer[Double] = ArrayBuffer(0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21....\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = Array.tabulate(100)(_.toDouble)\n",
    "val ab = new collection.mutable.ArrayBuffer[Double] ++ a\n",
    "ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Double = 4950.0\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.0 /: a)(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "The results from the algorithm need to restructured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vocab: Array[String] = Array(u, polic, govt, new, plan, man, council, iraq, sai, call, kill, win, charg, court, back, face, claim, urg, fund, report, warn, fire, nsw, boost, take, get, attack, set, death, water, qld, wa, mai, probe, group, consid, seek, crash, hospit, open, continu, world, health, concern, cup, miss, lead, hope, mp, pm, protest, meet, help, servic, test, two, murder, chang, hit, iraqi, australia, minist, ban, talk, drug, war, sydnei, home, vic, secur, sa, rise, support, year, top, industri, road, bomb, work, investig, final, nt, air, car, offer, welcom, return, fear, act, reject, jail, case, hous, forc, job, fight, record, make, strike, end, deal, defend, worker, arrest, pai, trial, australian, power, public, farmer, m, cut, move, rule, still, dai, dead, south, want, tr...\n"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocab = cv_model.vocabulary\n",
    "val topics = model.describeTopics()\n",
    "val topics_rdd = topics.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false),StructField(indices,ArrayType(IntegerType,true),true),StructField(scores,ArrayType(DoubleType,true),true))\n"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define the schema and make a data frame with it\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val schema = new StructType()\n",
    "  .add(StructField(\"id\", IntegerType, false))\n",
    "  .add(StructField(\"indices\", ArrayType(IntegerType, true)))\n",
    "  .add(StructField(\"scores\", ArrayType(DoubleType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- indices: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- scores: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "df1: org.apache.spark.sql.DataFrame = [id: int, indices: array<int> ... 1 more field]\n"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "// import org.apache.spark.sql.functions.explode\n",
    "\n",
    "val df1 = spark.createDataFrame(topics_rdd, schema)\n",
    "\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class tab1\n"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Using the column names create a case class that has the Arrays in it\n",
    "case class tab1(id: Int, indices: WrappedArray[Int], scores: WrappedArray[Double])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|             indices|              scores|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[300, 69, 359, 49...|[0.00273718541331...|\n",
      "|  1|[239, 369, 201, 3...|[0.00257811336156...|\n",
      "|  2|[298, 177, 155, 4...|[0.00268410608851...|\n",
      "|  3|[261, 491, 145, 2...|[0.00260131780330...|\n",
      "|  4|[54, 384, 348, 89...|[0.00956404662531...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.Dataset[tab1] = [id: int, indices: array<int> ... 1 more field]\n"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cast the dataframe to be of that type.\n",
    "val df2 = df1.as[tab1]\n",
    "df2.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3: List[scala.collection.mutable.WrappedArray[(String, Double)]] = List(WrappedArray((poll,0.0027371854133140853), (secur,0.002560246657611293), (raid,0.002551263934741385), (line,0.0025177467974942472), (bill,0.002502333762281136), (teacher,0.0024809462044122427), (race,0.002456433101087516), (act,0.0024250307641168), (nt,0.0024169064692088882), (author,0.002416614667940068)), WrappedArray((korea,0.002578113361563147), (nurs,0.002564425666431991), (doctor,0.002551590449154687), (increas,0.002520360217278145), (strike,0.00251809615524107), (poll,0.0025062644280818887), (british,0.0025029408660668896), (kill,0.002490891680355644), (brisban,0.0024788558521510163), (tip,0.002477245307779051)), WrappedArray((storm,0.0026841060885122938), (look,0.002533607378645535), (tour,0.00251219978686...\n"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Then the columns can be accessed as members and with types.\n",
    "// this uses the indices into the vocab \n",
    "\n",
    "val df3 = df2.map(x => x.indices.map(vocab).zip(x.scores)).collect.toList.map {\n",
    "    _.map(x => (x._1, x._2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::\n",
      "poll :: 0.0027371854133140853\n",
      "secur :: 0.002560246657611293\n",
      "raid :: 0.002551263934741385\n",
      "line :: 0.0025177467974942472\n",
      "bill :: 0.002502333762281136\n",
      "teacher :: 0.0024809462044122427\n",
      "race :: 0.002456433101087516\n",
      "act :: 0.0024250307641168\n",
      "nt :: 0.0024169064692088882\n",
      "author :: 0.002416614667940068\n",
      "::\n",
      "korea :: 0.002578113361563147\n",
      "nurs :: 0.002564425666431991\n",
      "doctor :: 0.002551590449154687\n",
      "increas :: 0.002520360217278145\n",
      "strike :: 0.00251809615524107\n",
      "poll :: 0.0025062644280818887\n",
      "british :: 0.0025029408660668896\n",
      "kill :: 0.002490891680355644\n",
      "brisban :: 0.0024788558521510163\n",
      "tip :: 0.002477245307779051\n",
      "::\n",
      "storm :: 0.0026841060885122938\n",
      "look :: 0.002533607378645535\n",
      "tour :: 0.0025121997868690366\n",
      "women :: 0.002503664017014095\n",
      "remain :: 0.002479214785423427\n",
      "top :: 0.0024574859047018613\n",
      "question :: 0.002451903299087713\n",
      "rais :: 0.0024474415348164436\n",
      "g :: 0.0024332839219606646\n",
      "busi :: 0.0024330620637832406\n",
      "::\n",
      "offic :: 0.002601317803300338\n",
      "post :: 0.0025859822436945655\n",
      "deni :: 0.0025354377938667898\n",
      "teacher :: 0.0024973075853049124\n",
      "rais :: 0.00246877846146971\n",
      "youth :: 0.002451744412811615\n",
      "bu :: 0.002450555161823485\n",
      "safeti :: 0.002417046425760324\n",
      "line :: 0.0024123531929994\n",
      "children :: 0.0024107722975154422\n",
      "::\n",
      "test :: 0.00956404662531811\n",
      "near :: 0.006006883514845379\n",
      "hill :: 0.0026802569212485768\n",
      "reject :: 0.0026525859055762705\n",
      "injuri :: 0.0026086423910744445\n",
      "wast :: 0.002577756770694025\n",
      "number :: 0.0025020135527626976\n",
      "woman :: 0.002488459380385237\n",
      "melbourn :: 0.0024613136917275937\n",
      "close :: 0.0024547922758129865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res149: List[scala.collection.mutable.WrappedArray[Unit]] = List(WrappedArray((), (), (), (), (), (), (), (), (), ()), WrappedArray((), (), (), (), (), (), (), (), (), ()), WrappedArray((), (), (), (), (), (), (), (), (), ()), WrappedArray((), (), (), (), (), (), (), (), (), ()), WrappedArray((), (), (), (), (), (), (), (), (), ()))\n"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.map(y => { println(\"::\"); y.map(x => println(x._1 + \" :: \" + x._2) ) } );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|  default|finaltable|      false|\n",
      "|         |    topics|       true|\n",
      "+---------+----------+-----------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show\n",
    "spark.sql(\"select count(*) from topics\").show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
