{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spylon Kernel Test with Spark 3.4.0\n",
    "\n",
    "This has been updated from Spark 2.4. I use a local SBT installation via /misc/build/0/classes. This is similar to the PySpark spark0 notebook.\n",
    "\n",
    "This must use the same Scala version as Spark - which is 2.13 (it was 2.11).\n",
    "\n",
    "I haven't recompiled the Scala source code in src - the artikus.spark classes.\n",
    "\n",
    "Once a Spark context is instantiated, it should be accessible from http://j1:4040 if the host of this notebook is j1. This hostname is spark.driver.host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Of no use for a Spylon notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Initialization of Spark\n",
    "\n",
    "Note that we can set things like driver memory etc.\n",
    "\n",
    "If `launcher._spark_home` is not set it will default to looking at the `SPARK_HOME` environment variable.\n",
    "\n",
    "I run on a cluster owned by the hadoop user who is a member of my group devel.\n",
    "\n",
    "I build new features for Scala and access them via /misc/build/0/classes. I have to restart the kernel to access any new classes. And must relaunch Spark to access changes.\n",
    "\n",
    "I can't change the spark.sql.warehouse.dir\n",
    "\n",
    "This loads external JARs - com.johnsnowlabs.nlp - and its dependencies with ivy. These go to .ivy2/cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master = \"yarn\"\n",
    "launcher.conf.spark.app.name = \"spark-lda\"\n",
    "launcher.conf.spark.executor.cores = 8\n",
    "launcher.num_executors = 4\n",
    "launcher.executor_cores = 4\n",
    "launcher.driver_memory = '4g'\n",
    "launcher.conf.set(\"spark.driver.cores\", 4);\n",
    "launcher.conf.set(\"spark.executor.cores\", 4);\n",
    "launcher.conf.set(\"spark.executor.memory\", \"4g\");\n",
    "launcher.conf.set(\"spark.executor.instances\", 4);\n",
    "launcher.conf.set(\"spark.sql.warehouse.dir\", \"file:///home/hadoop/data/hive\");\n",
    "launcher.conf.set(\"spark.sql.catalogImplementation\", \"hive\");\n",
    "launcher.conf.set(\"spark.hadoop.fs.permissions.umask-mode\", \"002\");\n",
    "launcher.conf.set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.2\");\n",
    "launcher.conf.set(\"spark.driver.extraClassPath\", \":/misc/build/0/classes/:/usr/share/java/postgresql.jar\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Spark Configuration\n",
    "\n",
    "Some basic operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://k1:8088/proxy/application_1684145519388_0001\n",
       "SparkContext available as 'sc' (version = 3.4.0, master = yarn, app id = application_1684145519388_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@228596c2\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark // spark is the SQL session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: String = 3.4.0\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.warehouse.dir --> file:/home/hadoop/data/hive\n",
      "spark.hadoop.fs.permissions.umask-mode --> 002\n",
      "spark.executor.extraJavaOptions --> -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.driver.host --> j1.host0\n",
      "spark.serializer.objectStreamReset --> 100\n",
      "spark.driver.port --> 42869\n",
      "spark.rdd.compress --> True\n",
      "spark.repl.class.uri --> spark://j1.host0:42869/classes\n",
      "spark.repl.class.outputDir --> /var/tmp/tmpbyipmdnf\n",
      "spark.app.name --> spylon-kernel\n",
      "spark.driver.memory --> 4g\n",
      "spark.executor.instances --> 4\n",
      "spark.submit.pyFiles --> \n",
      "spark.ui.showConsoleProgress --> true\n",
      "spark.app.submitTime --> 1684141882895\n",
      "spark.app.startTime --> 1684141883393\n",
      "spark.executor.id --> driver\n",
      "spark.driver.cores --> 4\n",
      "spark.driver.extraJavaOptions --> -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.submit.deployMode --> client\n",
      "spark.master --> local[*]\n",
      "spark.executor.memory --> 4g\n",
      "spark.driver.extraClassPath --> :/misc/build/0/classes/:/usr/share/java/postgresql.jar\n",
      "spark.sql.catalogImplementation --> hive\n",
      "spark.executor.cores --> 4\n",
      "spark.app.id --> local-1684141886448\n"
     ]
    }
   ],
   "source": [
    "spark.conf.getAll foreach (x => println(x._1 + \" --> \" + x._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------------------+--------------------+\n",
      "|   name|      catalog|         description|         locationUri|\n",
      "+-------+-------------+--------------------+--------------------+\n",
      "|default|spark_catalog|Default Hive data...|file:/misc/build/...|\n",
      "+-------+-------------+--------------------+--------------------+\n",
      "\n",
      "+-------------+-----------+\n",
      "|         name|description|\n",
      "+-------------+-----------+\n",
      "|spark_catalog|       null|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dbs1: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Database] = [name: string, catalog: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var dbs1 = spark.catalog.listDatabases()\n",
    "dbs1.show\n",
    "spark.catalog.listCatalogs().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d0: Array[org.apache.spark.sql.catalog.Database] = Array(Database[name='default', catalog='spark_catalog', description='Default Hive database', path='file:/misc/build/0/spark-eg0/spark-warehouse'])\n",
       "res4: String = file:/misc/build/0/spark-eg0/spark-warehouse\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d0 = spark.catalog.listDatabases().take(1)\n",
    "d0(0).locationUri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|  default|finaltable|      false|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df0: org.apache.spark.sql.DataFrame = [count(1): bigint]\n",
       "df0: org.apache.spark.sql.DataFrame = [count(1): bigint]\n",
       "df0: org.apache.spark.sql.DataFrame = [count(1): bigint]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var df0 = spark.sql(\"show databases\")\n",
    "df0.show()\n",
    "df0 = spark.sql(\"show tables\")\n",
    "df0.show()\n",
    "df0 = spark.sql(\"select count(*) from finalTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Using local Scala Builds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://j1.host0:4040\n",
       "SparkContext available as 'sc' (version = 3.4.0, master = local[*], app id = local-1684091628830)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import artikus.spark.U\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import artikus.spark.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.ClassCastException",
     "evalue": " class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')",
     "output_type": "error",
     "traceback": [
      "java.lang.ClassCastException: class jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to class java.net.URLClassLoader (jdk.internal.loader.ClassLoaders$AppClassLoader and java.net.URLClassLoader are in module java.base of loader 'bootstrap')",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "val cl = spark.getClass().getClassLoader()\n",
    "cl.asInstanceOf[java.net.URLClassLoader].getURLs.map(x => x.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.sql.SparkSession\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "// These are from the /misc/build/0/classes\n",
    "U.identity\n",
    "U.printClass(spark)\n",
    "U.alert(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.classes(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.flist(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SparkSession operations\n",
    "\n",
    "Basic operations\n",
    "https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkSession.html#createDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val strings = spark.emptyDataset[String]\n",
    "strings.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val one = spark.createDataset(Seq(1))\n",
    "one.show\n",
    "one.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use an implicit requires a \"spark\" in the namespace.\n",
    "import spark.implicits._\n",
    "\n",
    "val one = Seq(1).toDS\n",
    "one.show\n",
    "one.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Using spark.range()\n",
    "val range0 = spark.range(start = 0, end = 4, step = 2, numPartitions = 5)\n",
    "range0.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// More packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = Seq(\"a\", \"b\", \"c\", \"d\") zip (0 to 4)\n",
    "\n",
    "U.printClass(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = Seq(\"foo\", \"bar\", \"baz\") zip 1 :: 2 :: 3 :: Nil\n",
    "val data1 = Seq(\"foo\", \"bar\", \"bar\") zip 4 :: 5 :: 6 :: Nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds = spark.createDataset(data)\n",
    "\n",
    "val ds1 = sc.parallelize(data)\n",
    "\n",
    "U.printClass(ds)\n",
    "U.printClass(ds1)\n",
    "\n",
    "val ds2 = sc.parallelize(data1)\n",
    "\n",
    "ds1.join(ds2).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Local file URI\n",
    "// non-existent file loads\n",
    "// /misc/build/0/prog-scala-2nd-ed-code-examples\n",
    "val local2 = U.local1(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val f1 = \"rev-users.csv\"\n",
    "val file = sc.textFile(local2(f1).toString())\n",
    "U.printClass(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This file has a header row\n",
    "// Take the first row, index into it, split and return a sequence\n",
    "val h2 = file.take(1)(0).split(\",\").toSeq\n",
    "\n",
    "// Get the remainder by using subtract\n",
    "// convert the header row back to an RDD using parallelize\n",
    "val r1 = file.subtract(sc.parallelize(file.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Look at the underlying row\n",
    "r1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now map over the quantities\n",
    "// The transformations are only applied when we take(), use the column names from h2.\n",
    "val df0 = r1.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b.toInt,c,d,e,f.toInt,g,h,i,j.toInt,k.toInt,l.toInt)}.toDF(h2:_*)\n",
    "df0.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val f2 = \"rev-devices.csv\"\n",
    "val file2 = sc.textFile(local2(f2).toString())\n",
    "U.printClass(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// But error results here if file does not exist\n",
    "// Or returns empty array if it is empty\n",
    "val lens = file.map(s => s.length)\n",
    "file.take(5)\n",
    "lens.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val x0 = file.take(1)\n",
    "\n",
    "// Some arbitrary file processing - append a number to each line\n",
    "val pairs = file.map(s => (s, 911))\n",
    "val counts = pairs.reduceByKey((a, b) => a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val counts1 = counts.repartition(1)\n",
    "\n",
    "U.rmdir(\"counts1\")\n",
    "counts1.saveAsTextFile(local2(\"counts1\").toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pairs = file.map(x => (x.split(\",\")(0), x))\n",
    "\n",
    "val pairs1 = pairs.join(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Make some (K, V) tuples\n",
    "\n",
    "println(x0(0))\n",
    "\n",
    "val x1 = x0(0).split(\",\").toSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df0 = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b,c,d,e,f,g,h,i,j,k,l)}.toDF(x1:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The x1:_* is to be preferred to this\n",
    "\n",
    "// val fileToDf = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "// (a,b,c,d,e,f,g,h,i,j,k,l)}.toDF(\"user_id\", \"birth_year\", \"country\", \"city\", \"created_date\", \"user_settings_crypto_unlocked\", \"plan\", \"attributes_notifications_marketing_push\", \"attributes_notifications_marketing_email\", \"num_contacts\", \"num_referrals\", \"num_successful_referrals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df0 = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b.toInt,c,d,e,f,g,h,i,j,k,l)}.toDF(x1:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileToDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.map(_.split(\",\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df1 = file.subtract(sc.parallelize(file.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.printClass(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(f1:String, sep:String)(implicit sc: org.apache.spark.SparkContext) : org.apache.spark.rdd.RDD[String] = {\n",
    "    val f = sc.textFile(f1)\n",
    "    return f\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split(local2(f1).toString(), \",\")(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.printClass(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLLib\n",
    "\n",
    "Using LDA.\n",
    "\n",
    "https://medium.com/analytics-vidhya/distributed-topic-modelling-using-spark-nlp-and-spark-mllib-lda-6db3f06a4da3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://k1:8088/proxy/application_1684147981528_0001\n",
       "SparkContext available as 'sc' (version = 3.4.0, master = yarn, app id = application_1684147981528_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "url: String = file:///a/l/X-image/cache/data/abcnews-date-text.csv\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val url = \"file:///a/l/X-image/cache/data/abcnews-date-text.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type0: String = csv\n",
       "infer_schema: String = true\n",
       "first_row_is_header: String = true\n",
       "delimiter: String = ,\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val type0=\"csv\"\n",
    "val infer_schema=\"true\"\n",
    "val first_row_is_header = \"true\"\n",
    "val delimiter=\",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df0: org.apache.spark.sql.DataFrame = [publish_date: int, headline_text: string]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df0 = spark.read.format(type0)\n",
    "  .option(\"inferSchema\", infer_schema)\n",
    "  .option(\"header\", first_row_is_header)\n",
    "  .option(\"sep\", delimiter)\n",
    "  .load(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Long = 1082168\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import com.johnsnowlabs.nlp.DocumentAssembler\n",
       "import com.johnsnowlabs.nlp.annotators.Tokenizer\n",
       "import com.johnsnowlabs.nlp.annotators.Normalizer\n",
       "import com.johnsnowlabs.nlp.annotators.StopWordsCleaner\n",
       "import com.johnsnowlabs.nlp.annotators.Stemmer\n",
       "import com.johnsnowlabs.nlp.Finisher\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.johnsnowlabs.nlp.DocumentAssembler\n",
    "import com.johnsnowlabs.nlp.annotators.Tokenizer\n",
    "import com.johnsnowlabs.nlp.annotators.Normalizer\n",
    "import com.johnsnowlabs.nlp.annotators.StopWordsCleaner\n",
    "import com.johnsnowlabs.nlp.annotators.Stemmer\n",
    "import com.johnsnowlabs.nlp.Finisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document_assembler: com.johnsnowlabs.nlp.DocumentAssembler = document_c879f8ee263e\n",
       "tokenizer: com.johnsnowlabs.nlp.annotators.Tokenizer = REGEX_TOKENIZER_84c9b7813d48\n",
       "normalizer: com.johnsnowlabs.nlp.annotators.Normalizer = NORMALIZER_37d0d62bb66f\n",
       "stopwords_cleaner: com.johnsnowlabs.nlp.annotators.StopWordsCleaner = STOPWORDS_CLEANER_e6ad3685cf9e\n",
       "stemmer: com.johnsnowlabs.nlp.annotators.Stemmer = STEMMER_075cdcf04187\n",
       "finisher: com.johnsnowlabs.nlp.Finisher = finisher_02ed352a6956\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Split sentence to tokens(array)\n",
    "val document_assembler = new DocumentAssembler().setInputCol(\"headline_text\").setOutputCol(\"document\").setCleanupMode(\"shrink\") \n",
    "\n",
    "// clean unwanted characters and garbage\n",
    "val tokenizer = new Tokenizer().setInputCols(Array(\"document\")).setOutputCol(\"token\")\n",
    "\n",
    "val normalizer = new Normalizer().setInputCols(Array(\"token\")).setOutputCol(\"normalized\")\n",
    "\n",
    "// remove stopwords\n",
    "val stopwords_cleaner = new StopWordsCleaner().setInputCols(\"normalized\").setOutputCol(\"cleanTokens\").setCaseSensitive(false)\n",
    "\n",
    "// stem the words to bring them to the root form.\n",
    "val stemmer = new Stemmer().setInputCols(Array(\"cleanTokens\")).setOutputCol(\"stem\")\n",
    "\n",
    "// Finisher is the most important annotator. \n",
    "// Spark NLP adds its own structure when we convert each row in the dataframe to document. \n",
    "// Finisher helps us to bring back the expected structure viz. array of tokens.\n",
    "val finisher = new Finisher().setInputCols(Array(\"stem\")).setOutputCols(Array(\"tokens\"))\n",
    "    .setOutputAsArray(true).setCleanAnnotations(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable] = Array(document_c879f8ee263e, REGEX_TOKENIZER_84c9b7813d48, NORMALIZER_37d0d62bb66f, STOPWORDS_CLEANER_e6ad3685cf9e, STEMMER_075cdcf04187, finisher_02ed352a6956)\n",
       "nlp_pipeline: org.apache.spark.ml.Pipeline = pipeline_80f149cb34b5\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We build a ml pipeline so that each phase can be executed in sequence. \n",
    "// This pipeline can also be used to test the model. \n",
    "// train the pipeline\n",
    "val stages=Array(document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, finisher)\n",
    "val nlp_pipeline = new Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nlp_model: org.apache.spark.ml.PipelineModel = pipeline_80f149cb34b5\n",
       "processed_df0: org.apache.spark.sql.DataFrame = [publish_date: int, headline_text: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//  apply the pipeline to transform dataframe.\n",
    "val nlp_model = nlp_pipeline.fit(df0) \n",
    "val processed_df0  = nlp_model.transform(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|publish_date|              tokens|\n",
      "+------------+--------------------+\n",
      "|    20030219|[aba, decid, comm...|\n",
      "|    20030219|[act, fire, wit, ...|\n",
      "|    20030219|[g, call, infrast...|\n",
      "|    20030219|[air, nz, staff, ...|\n",
      "|    20030219|[air, nz, strike,...|\n",
      "|    20030219|[ambiti, olsson, ...|\n",
      "|    20030219|[antic, delight, ...|\n",
      "|    20030219|[aussi, qualifi, ...|\n",
      "|    20030219|[aust, address, u...|\n",
      "|    20030219|[australia, lock,...|\n",
      "|    20030219|[australia, contr...|\n",
      "|    20030219|[barca, take, rec...|\n",
      "|    20030219|[bathhous, plan, ...|\n",
      "|    20030219|[big, hope, launc...|\n",
      "|    20030219|[big, plan, boost...|\n",
      "|    20030219|[blizzard, buri, ...|\n",
      "|    20030219|[brigadi, dismiss...|\n",
      "|    20030219|[british, combat,...|\n",
      "|    20030219|[bryant, lead, la...|\n",
      "|    20030219|[bushfir, victim,...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokens_df0: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [publish_date: int, tokens: array<string>]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//  nlp pipeline create intermediary columns that we dont need. So lets select the columns that we need\n",
    "val tokens_df0 = processed_df0.select(\"publish_date\",\"tokens\").limit(10000)\n",
    "tokens_df0.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "To generate features from textual data. Latent Dirichlet Allocation requires a data-specific vocabulary to perform topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.CountVectorizer\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cv: org.apache.spark.ml.feature.CountVectorizer = cntVec_91a623e3720b\n",
       "cv_model: org.apache.spark.ml.feature.CountVectorizerModel = CountVectorizerModel: uid=cntVec_91a623e3720b, vocabularySize=500\n",
       "vectorized_tokens: org.apache.spark.sql.DataFrame = [publish_date: int, tokens: array<string> ... 1 more field]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cv = new CountVectorizer().setInputCol(\"tokens\").setOutputCol(\"features\").setVocabSize(500).setMinTF(3.0)\n",
    "\n",
    "// train the model\n",
    "val cv_model = cv.fit(tokens_df0)\n",
    "// transform the data. Output column name will be features.\n",
    "val vectorized_tokens = cv_model.transform(tokens_df0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.clustering.LDA\n",
       "import org.apache.spark.sql.Row\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.clustering.LDA\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_topics: Int = 3\n",
       "lda: org.apache.spark.ml.clustering.LDA = lda_310b7cc9f6b5\n",
       "model: org.apache.spark.ml.clustering.LDAModel = LocalLDAModel: uid=lda_310b7cc9f6b5, k=3, numFeatures=500\n",
       "ll: Double = -443.6715451674518\n",
       "lp: Double = 147.89051505581727\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val num_topics = 3\n",
    "val lda = new LDA().setK(num_topics).setMaxIter(10)\n",
    "val model = lda.fit(vectorized_tokens)\n",
    "\n",
    "val ll = model.logLikelihood(vectorized_tokens)\n",
    "val lp = model.logPerplexity(vectorized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -443.6715451674518\n",
      "The upper bound on perplexity: 147.89051505581727\n"
     ]
    }
   ],
   "source": [
    "println(\"The lower bound on the log likelihood of the entire corpus: \" + ll.toString())\n",
    "println(\"The upper bound on perplexity: \" + lp.toString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "The results from the algorithm need to restructured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vocab: Array[String] = Array(u, iraq, war, polic, govt, man, plan, new, sai, iraqi, council, win, fire, claim, call, charg, protest, warn, kill, report, mai, back, nsw, baghdad, urg, world, court, face, death, fund, water, anti, take, troop, get, crash, forc, cup, continu, qld, set, hope, rain, un, attack, mp, pm, hospit, meet, miss, open, concern, lead, bomb, hit, wa, ban, australia, aust, boost, two, air, final, support, group, murder, health, seek, secur, deni, vic, probe, missil, coast, farmer, car, end, welcom, consid, fight, year, investig, sa, drought, move, still, offer, elect, help, green, first, sar, union, australian, saddam, home, minist, korea, woman, dead, go, coalit, defend, oil, howard, chang, case, make, arrest, work, begin, jail, top, hous, strike, act, race, rise, com...\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocab = cv_model.vocabulary\n",
    "val topics = model.describeTopics()\n",
    "val topics_rdd = topics.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false),StructField(indices,ArrayType(IntegerType,true),true),StructField(scores,ArrayType(DoubleType,true),true))\n"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define the schema and make a data frame with it\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val schema = new StructType()\n",
    "  .add(StructField(\"id\", IntegerType, false))\n",
    "  .add(StructField(\"indices\", ArrayType(IntegerType, true)))\n",
    "  .add(StructField(\"scores\", ArrayType(DoubleType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- indices: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- scores: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "df1: org.apache.spark.sql.DataFrame = [id: int, indices: array<int> ... 1 more field]\n"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "// import org.apache.spark.sql.functions.explode\n",
    "\n",
    "val df1 = spark.createDataFrame(topics_rdd, schema)\n",
    "\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class tab1\n"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Using the column names create a case class that has the Arrays in it\n",
    "case class tab1(id: Int, indices: WrappedArray[Int], scores: WrappedArray[Double])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.Dataset[tab1] = [id: int, indices: array<int> ... 1 more field]\n"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Cast the dataframe to be of that type.\n",
    "val df2 = df1.as[tab1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3: List[scala.collection.mutable.WrappedArray[(String, Double)]] = List(WrappedArray((candid,0.002735799497557659), (deni,0.0025591722001258605), (near,0.0025502052891865985), (nat,0.002516747150826247), (ga,0.002501361246567455), (develop,0.002480011336368051), (east,0.002455541382486463), (help,0.0024241943218035176), (investig,0.0024160843277695725), (leagu,0.0024157930401469477)), WrappedArray((test,0.009028235749467671), (accid,0.0025592625162857503), (franc,0.0025456931174624147), (labor,0.002532968828928002), (women,0.00250200850524915), (woman,0.002499764010434832), (candid,0.0024880345393256766), (rate,0.0024847397012931613), (council,0.002472794651023874), (crisi,0.0024608628428185626)), WrappedArray((low,0.002682624379971457), (ta,0.0025324007707322785), (trial,0.0025110323...\n"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Then the columns can be accessed as members and with types.\n",
    "// this uses the indices into the vocab \n",
    "\n",
    "val df3 = df2.map(x => x.indices.map(vocab).zip(x.scores)).collect.toList.map {\n",
    "    _.map(x => (x._1, x._2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::\n",
      "candid :: 0.002735799497557659\n",
      "deni :: 0.0025591722001258605\n",
      "near :: 0.0025502052891865985\n",
      "nat :: 0.002516747150826247\n",
      "ga :: 0.002501361246567455\n",
      "develop :: 0.002480011336368051\n",
      "east :: 0.002455541382486463\n",
      "help :: 0.0024241943218035176\n",
      "investig :: 0.0024160843277695725\n",
      "leagu :: 0.0024157930401469477\n",
      "::\n",
      "test :: 0.009028235749467671\n",
      "accid :: 0.0025592625162857503\n",
      "franc :: 0.0025456931174624147\n",
      "labor :: 0.002532968828928002\n",
      "women :: 0.00250200850524915\n",
      "woman :: 0.002499764010434832\n",
      "candid :: 0.0024880345393256766\n",
      "rate :: 0.0024847397012931613\n",
      "council :: 0.002472794651023874\n",
      "crisi :: 0.0024608628428185626\n",
      "::\n",
      "low :: 0.002682624379971457\n",
      "ta :: 0.0025324007707322785\n",
      "trial :: 0.0025110323104668604\n",
      "championship :: 0.0025025121433745346\n",
      "send :: 0.0024781076031899284\n",
      "farmer :: 0.002456418441272121\n",
      "question :: 0.0024508460402524797\n",
      "doubl :: 0.00244639243175977\n",
      "shoot :: 0.0024322606979841842\n",
      "season :: 0.0024320392453472716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res138: List[scala.collection.mutable.WrappedArray[Unit]] = List(WrappedArray((), (), (), (), (), (), (), (), (), ()), WrappedArray((), (), (), (), (), (), (), (), (), ()), WrappedArray((), (), (), (), (), (), (), (), (), ()))\n"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.map(y => { println(\"::\"); y.map(x => println(x._1 + \" :: \" + x._2) ) } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|  default|finaltable|      false|\n",
      "|         |    topics|       true|\n",
      "+---------+----------+-----------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show\n",
    "spark.sql(\"select count(*) from topics\").show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel~"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
