{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec: PySpark using Spark and Hadoop 3\n",
    "\n",
    "This was previously running on 2.4.4. See also the spark-util notebook.\n",
    "\n",
    "This can use a Yarn cluster or the master directly. Yarn can use the local file system if it is specified by a URI.\n",
    "\n",
    "Hive should be used and there are extra paths for classes and Database drivers added. \n",
    "\n",
    "The k1 host has systemd services to start hadoop and hive under a spark.target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows all the results in a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Application.log_level=\"WARN\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import urllib.request\n",
    "\n",
    "import findspark\n",
    "findspark.init() # You need to init before you can import the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/misc/build/1/hadoop/etc/hadoop/hadoop'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/misc/build/1/hadoop/etc/spark/conf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'.:/home/weaves/.local/bin:/home/weaves/bin:/misc/conda/condabin:/misc/conda/envs/toot/bin:/misc/conda/bin:/a/hypr/home/hypr/disk01/W-media/cache/sdkman/candidates/scala/current/bin:/a/hypr/home/hypr/disk01/W-media/cache/sdkman/candidates/sbt/current/bin:/a/hypr/home/hypr/disk01/W-media/cache/sdkman/candidates/maven/current/bin:/a/hypr/home/hypr/disk01/W-media/cache/sdkman/candidates/kotlin/current/bin:/a/hypr/home/hypr/disk01/W-media/cache/sdkman/candidates/java/current/bin:/a/hypr/home/hypr/disk01/W-media/cache/sdkman/candidates/gradle/current/bin:/snap/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/libexec:/usr/lib/git-core:/usr/local/games:/usr/games:/misc/sistemoj/share/bin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/misc/build/1/hadoop/etc/hadoop/hadoop:/misc/share/1/hadoop-3.3.5/share/hadoop/common/lib/*:/misc/share/1/hadoop-3.3.5/share/hadoop/common/*:/misc/share/1/hadoop-3.3.5/share/hadoop/hdfs:/misc/share/1/hadoop-3.3.5/share/hadoop/hdfs/lib/*:/misc/share/1/hadoop-3.3.5/share/hadoop/hdfs/*:/misc/share/1/hadoop-3.3.5/share/hadoop/mapreduce/*:/misc/share/1/hadoop-3.3.5/share/hadoop/yarn:/misc/share/1/hadoop-3.3.5/share/hadoop/yarn/lib/*:/misc/share/1/hadoop-3.3.5/share/hadoop/yarn/*'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['HADOOP_CONF_DIR']\n",
    "os.environ['SPARK_CONF_DIR']\n",
    "os.environ['PATH']\n",
    "os.environ['SPARK_DIST_CLASSPATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'logging' from '/misc/conda/envs/toot/lib/python3.9/logging/__init__.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload  # Not needed in Python 2\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.WARN, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "useYARN = False\n",
    "useYARN = True\n",
    "\n",
    "# YARN - test which access will work.\n",
    "cstr = \"spark://k1:7077\"\n",
    "if useYARN:\n",
    "    cstr = \"yarn\"\n",
    "\n",
    "# The HDFS url needs to be put using the hdfs tool. The HTTP one needs the web-server on (flora) j1\n",
    "# the file url does works for YARN and master-slave.\n",
    "url = \"hdfs:///weaves/input/text8\"\n",
    "url = \"http://flora/cache/text/text\"\n",
    "url = \"file:///a/l/X-image/cache/text/text8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"word2vec\").setMaster(cstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'word2vec'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.driver.cores', '4'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.executor.instances', '4'),\n",
       " ('spark.sql.warehouse.dir', 'file:///home/hadoop/data/hive'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.hadoop.fs.permissions.umask-mode', '002'),\n",
       " ('spark.driver.extraClassPath',\n",
       "  ':/misc/build/0/classes/:/usr/share/java/postgresql.jar')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the semi-colon should suppress the output, but doesn't with the Interactivity setting above, so x is used.\n",
    "x = conf.set(\"spark.driver.cores\", 4);\n",
    "x = conf.set(\"spark.driver.memory\", \"4g\");\n",
    "x = conf.set(\"spark.executor.cores\", 4);\n",
    "x = conf.set(\"spark.executor.memory\", \"4g\");\n",
    "x = conf.set(\"spark.executor.instances\", 4);\n",
    "x = conf.set(\"spark.sql.warehouse.dir\", \"file:///home/hadoop/data/hive\");\n",
    "x = conf.set(\"spark.sql.catalogImplementation\", \"hive\");\n",
    "x = conf.set(\"spark.hadoop.fs.permissions.umask-mode\", \"002\");\n",
    "x = conf.set(\"spark.driver.extraClassPath\", \":/misc/build/0/classes/:/usr/share/java/postgresql.jar\");\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Instantiation\n",
    "\n",
    "Create the Spark session and check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.instances', '4'),\n",
       " ('spark.driver.cores', '4'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.hadoop.fs.permissions.umask-mode', '002'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.driver.extraClassPath',\n",
       "  ':/misc/build/0/classes/:/usr/share/java/postgresql.jar'),\n",
       " ('spark.driver.port', '35373'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1684424921355_0001'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9.7-src.zip'),\n",
       " ('spark.driver.host', 'j1.host0'),\n",
       " ('spark.app.name', 'word2vec'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.id', 'application_1684424921355_0001'),\n",
       " ('spark.sql.warehouse.dir', 'file:///home/hadoop/data/hive'),\n",
       " ('spark.app.startTime', '1684570630102'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://k1:8088/proxy/application_1684424921355_0001'),\n",
       " ('spark.app.submitTime', '1684570629956'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.appUIAddress', 'http://j1.host0:4041'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'k1'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# that configuration should now be in the spark context.\n",
    "spark.sparkContext.getConf().getAll() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://j1.host0:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>word2vec</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=word2vec>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And it can be checked in the Web UI on the Environment tab. \n",
    "# The executors tab will only one\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to copy to a local file - not useful on a Spark Yarn cluster\n",
    "# with urllib.request.urlopen(url) as response:\n",
    "#    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "#        shutil.copyfileobj(response, tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive\n",
    "\n",
    "Load a file and send it to Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a lazy instruction and actions nothing on the executors\n",
    "rdd0 = sc.textFile(url).map(lambda row: row.split(\" \"))\n",
    "type(rdd0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "| anarchism origin...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternatively read as a dataframe, this will appear in thhe SQL/Dataframe tab. This is an immmediate operation.\n",
    "df0 = spark.read.text(url)\n",
    "df0.printSchema()\n",
    "df0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd0)\n",
    "type(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql(\"show databases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the finalTable should persist between invocations of Hive\n",
    "\n",
    "df1 = spark.sql(\"show tables\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "| anarchism origin...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0 = spark.read.text(url)\n",
    "df0.printSchema()\n",
    "df0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.createOrReplaceTempView(\"tempTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists finalTable\")\n",
    "spark.sql(\"create table finalTable AS select * from tempTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n",
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|  default|finaltable|      false|\n",
      "|  default|    xusers|      false|\n",
      "|         | temptable|       true|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0 = spark.sql(\"show databases\")\n",
    "df0.show()\n",
    "df0 = spark.sql(\"show tables\")\n",
    "df0.show()\n",
    "df0 = spark.sql(\"select count(*) from finalTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLLib Method\n",
    "\n",
    "Demonstrate access to an MLLib method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a 45 minutes for a 100 MByte file.\n",
    "word2Vec = Word2Vec()\n",
    "model = word2Vec.fit(rdd0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = model.findSynonyms('iran', 40)\n",
    "type(synonyms)\n",
    "syns0=list(synonyms)\n",
    "len(syns0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = model.findSynonyms('insensible', 40)\n",
    "type(synonyms)\n",
    "syns0=list(synonyms)\n",
    "len(syns0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = model.findSynonyms('sensible', 40)\n",
    "type(synonyms)\n",
    "syns0=list(synonyms)\n",
    "len(syns0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__dir__()\n",
    "len(model.getVectors())\n",
    "model.__dir__()\n",
    "## This shows it isn't of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sample of words\n",
    "vv=list(model.getVectors())\n",
    "s0=vv[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors used to transform the first word of the sample\n",
    "v0 = model.transform(s0[0])\n",
    "len(v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonyms to china by cosine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(syns0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# synonyms are easily captured. This is proximity to 'china'\n",
    "l0 = pd.DataFrame.from_records(list(syns0), columns =['L', 'R'])\n",
    "print(l0.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = model.findSynonyms(l0.iloc[0].loc['L'], 40)\n",
    "syns0 = list(synonyms)[1:10]\n",
    "l0 = pd.DataFrame.from_records(list(syns0), columns =['L', 'R']) \n",
    "print(l0.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = model.findSynonyms(l0.iloc[0].loc['L'], 1)\n",
    "next(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0.iloc[0].loc['L']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0[['L']]\n",
    "l0[['R']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"create table finalTable as select * from tempTable\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
