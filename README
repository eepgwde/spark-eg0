* Preamble

weaves

* Spark development

** Spylon

Accessing Spark through a notebook.

Spark is built with Scala 2.11 if you want to use your own SBT build, make
sure the Scala you use is the same.

*** Installing for Jupyter

**** Client setup

You need not be the hadoop user, but you must share the environment.

Within your Anaconda, install these.

 findspark (PySpark)
 spylon (Spylon - Scala)
 
And spylon needs another setup stage

 python3 -m spylon_kernel install

**** Cluster - Hadoop administrator

Start the Spark cluster as hadoop@k1

 hd_ sys start

**** Client configuration

Go into the Anaconda Python environment, with my script dconda

Then configure oneself as a client using the Hadoop account's definitions

 . ~hadoop/etc/hd_.def

You can also use your own configuration by overriding variables.

 export HADOOP_CONF_DIR=/misc/build/0/spark-srcs/etc/conf

And should you need to use Spark commands in your client shell, then add these paths.

 export PATH=$PATH:$(find $SPARK_HOME/{sbin,bin} $HDP0/{sbin,bin}  -type d 2> /dev/null | xargs | sed 's/ /:/g')

**** Client Application

And finally  jupyter notebook or jupyter lab

* Scala

Use the notebook spark-util.ipynb as a guide. Note the use of
/misc/build/0/classes. If you change the SBT build environment you need to
keep this link up to date to access artikus.spark

* PySpark

spark0.ipynb spark1.ipynb

Then spark0.ipynb will load and run.

spark0 is a Word2Vec demo.

* Postamble

This file's Emacs file variables

[  Local Variables: ]
[  mode:text ]
[  mode:outline-minor ]
[  mode:auto-fill ]
[  fill-column: 75 ]
[  coding: utf-8 ]
[  comment-column:50 ]
[  comment-start: "[  "  ]
[  comment-end:"]" ]
[  End: ]

