{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spylon Kernel Test with Spark 3.4.0\n",
    "\n",
    "This has been updated from Spark 2.4. I use a local SBT installation via /misc/build/0/classes.\n",
    "\n",
    "This must use the same Scala version as Spark - which is 2.13 (it was 2.11).\n",
    "\n",
    "I haven't recompiled the Scala source code in src - the artikus.spark classes.\n",
    "\n",
    "Once a Spark context is instantiated, it should be accessible from http://j1:4040 if the host of this notebook is j1. This hostname is spark.driver.host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of the spark context\n",
    "\n",
    "Note that we can set things like driver memory etc.\n",
    "\n",
    "If `launcher._spark_home` is not set it will default to looking at the `SPARK_HOME` environment variable.\n",
    "\n",
    "I run on a cluster owned by the hadoop user who is a member of my group devel.\n",
    "\n",
    "I build new features for Scala and access them via /misc/build/0/classes. I have to restart the kernel to access any new classes. And must relaunch Spark to access changes.\n",
    "\n",
    "I can't change the spark.sql.warehouse.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.num_executors = 4\n",
    "launcher.executor_cores = 2\n",
    "launcher.driver_memory = '4g'\n",
    "launcher.conf.set(\"spark.sql.warehouse.dir\", \"file:/misc/build/0/spark-eg0/spark-warehouse\")\n",
    "launcher.conf.set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "launcher.conf.set(\"spark.hadoop.fs.permissions.umask-mode\", \"002\")\n",
    "launcher.conf.set(\"spark.driver.extraClassPath\", \":/misc/build/0/classes/:/usr/share/java/postgresql.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import artikus.spark.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cl = spark.getClass().getClassLoader()\n",
    "cl.asInstanceOf[java.net.URLClassLoader].getURLs.map(x => x.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// These are from the /misc/build/0/classes\n",
    "U.identity\n",
    "U.printClass(spark)\n",
    "U.alert(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.classes(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.flist(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Of no use for a Spylon notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Configuration\n",
    "\n",
    "Some basic operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark // spark is the SQL session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.getAll foreach (x => println(x._1 + \" --> \" + x._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dbs = spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.printClass(dbs)\n",
    "dbs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val d0 = spark.catalog.listDatabases().take(1)\n",
    "d0(0).locationUri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession operations\n",
    "\n",
    "Basic operations\n",
    "https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-SparkSession.html#createDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val strings = spark.emptyDataset[String]\n",
    "strings.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val one = spark.createDataset(Seq(1))\n",
    "one.show\n",
    "one.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Use an implicit requires a \"spark\" in the namespace.\n",
    "import spark.implicits._\n",
    "\n",
    "val one = Seq(1).toDS\n",
    "one.show\n",
    "one.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Using spark.range()\n",
    "val range0 = spark.range(start = 0, end = 4, step = 2, numPartitions = 5)\n",
    "range0.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// More packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = Seq(\"a\", \"b\", \"c\", \"d\") zip (0 to 4)\n",
    "\n",
    "U.printClass(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = Seq(\"foo\", \"bar\", \"baz\") zip 1 :: 2 :: 3 :: Nil\n",
    "val data1 = Seq(\"foo\", \"bar\", \"bar\") zip 4 :: 5 :: 6 :: Nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ds = spark.createDataset(data)\n",
    "\n",
    "val ds1 = sc.parallelize(data)\n",
    "\n",
    "U.printClass(ds)\n",
    "U.printClass(ds1)\n",
    "\n",
    "val ds2 = sc.parallelize(data1)\n",
    "\n",
    "ds1.join(ds2).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Local file URI\n",
    "// non-existent file loads\n",
    "// /misc/build/0/prog-scala-2nd-ed-code-examples\n",
    "val local2 = U.local1(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val f1 = \"rev-users.csv\"\n",
    "val file = sc.textFile(local2(f1).toString())\n",
    "U.printClass(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// This file has a header row\n",
    "// Take the first row, index into it, split and return a sequence\n",
    "val h2 = file.take(1)(0).split(\",\").toSeq\n",
    "\n",
    "// Get the remainder by using subtract\n",
    "// convert the header row back to an RDD using parallelize\n",
    "val r1 = file.subtract(sc.parallelize(file.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Look at the underlying row\n",
    "r1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now map over the quantities\n",
    "// The transformations are only applied when we take(), use the column names from h2.\n",
    "val df0 = r1.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b.toInt,c,d,e,f.toInt,g,h,i,j.toInt,k.toInt,l.toInt)}.toDF(h2:_*)\n",
    "df0.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val f2 = \"rev-devices.csv\"\n",
    "val file2 = sc.textFile(local2(f2).toString())\n",
    "U.printClass(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// But error results here if file does not exist\n",
    "// Or returns empty array if it is empty\n",
    "val lens = file.map(s => s.length)\n",
    "file.take(5)\n",
    "lens.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val x0 = file.take(1)\n",
    "\n",
    "// Some arbitrary file processing - append a number to each line\n",
    "val pairs = file.map(s => (s, 911))\n",
    "val counts = pairs.reduceByKey((a, b) => a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val counts1 = counts.repartition(1)\n",
    "\n",
    "U.rmdir(\"counts1\")\n",
    "counts1.saveAsTextFile(local2(\"counts1\").toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pairs = file.map(x => (x.split(\",\")(0), x))\n",
    "\n",
    "val pairs1 = pairs.join(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Make some (K, V) tuples\n",
    "\n",
    "println(x0(0))\n",
    "\n",
    "val x1 = x0(0).split(\",\").toSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df0 = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b,c,d,e,f,g,h,i,j,k,l)}.toDF(x1:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The x1:_* is to be preferred to this\n",
    "\n",
    "// val fileToDf = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "// (a,b,c,d,e,f,g,h,i,j,k,l)}.toDF(\"user_id\", \"birth_year\", \"country\", \"city\", \"created_date\", \"user_settings_crypto_unlocked\", \"plan\", \"attributes_notifications_marketing_push\", \"attributes_notifications_marketing_email\", \"num_contacts\", \"num_referrals\", \"num_successful_referrals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df0 = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b.toInt,c,d,e,f,g,h,i,j,k,l)}.toDF(x1:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileToDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.map(_.split(\",\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df1 = file.subtract(sc.parallelize(file.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.printClass(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(f1:String, sep:String)(implicit sc: org.apache.spark.SparkContext) : org.apache.spark.rdd.RDD[String] = {\n",
    "    val f = sc.textFile(f1)\n",
    "    return f\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split(local2(f1).toString(), \",\")(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.printClass(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
