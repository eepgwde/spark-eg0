{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spylon Kernel Test with Spark 2.4.4\n",
    "\n",
    "I use a local SBT installation via /misc/build/0/classes.\n",
    "\n",
    "This must use the same Scala version as Spark - which is 2.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of the spark context\n",
    "\n",
    "Note that we can set things like driver memory etc.\n",
    "\n",
    "If `launcher._spark_home` is not set it will default to looking at the `SPARK_HOME` environment variable.\n",
    "\n",
    "I run on a cluster owned by hadoop in group devel, so change the umask.\n",
    "\n",
    "I build new features for Scala and access them via /misc/build/0/classes. I have to restart the kernel and relaunch Spark to access the new classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.num_executors = 4\n",
    "launcher.executor_cores = 2\n",
    "launcher.driver_memory = '4g'\n",
    "launcher.conf.set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "launcher.conf.set(\"spark.hadoop.fs.permissions.umask-mode\", \"002\")\n",
    "launcher.conf.set(\"spark.driver.extraClassPath\", \":/usr/share/java/postgresql.jar:/misc/build/0/classes/\")\n",
    "launcher.conf.set(\"spark.executor.extraClassPath\", \":/usr/share/java/postgresql.jar:/misc/build/0/classes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://k1:8088/proxy/application_1578316620687_0001\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = yarn, app id = application_1578316620687_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import artikus.spark.U\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import artikus.spark.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cl: ClassLoader = sun.misc.Launcher$AppClassLoader@7106e68e\n",
       "res0: Array[String] = Array(file:/usr/share/java/postgresql.jar, file:/misc/build/0/prog-scala-2nd-ed-code-examples/target/scala-2.11/classes/, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/conf/, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/commons-net-3.1.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/jsr305-1.3.9.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/logging-interceptor-3.12.0.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/generex-1.0.1.jar, file:/misc/s..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cl = spark.getClass().getClassLoader()\n",
    "cl.asInstanceOf[java.net.URLClassLoader].getURLs.map(x => x.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.sql.SparkSession\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "U.identity\n",
    "U.printClass(spark)\n",
    "U.alert(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Seq[String] = ArraySeq(file:/usr/share/java/postgresql.jar, file:/misc/build/0/prog-scala-2nd-ed-code-examples/target/scala-2.11/classes/, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/conf/, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/commons-net-3.1.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/jsr305-1.3.9.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/logging-interceptor-3.12.0.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/generex-1.0.1.jar, file:/misc/share/0/spark-2.4.4-bin-hadoop2.7/jars/hadoop-mapreduce-clie..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.classes(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: List[java.io.File] = List(./spark-util.ipynb, ./README, ./build.sbt, ./spark0.ipynb, ./.gitmodules, ./.Rhistory, ./.emacs.desktop, ./flights.csv, ./spark1.ipynb)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.flist(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: List[java.io.File] = List(./spark-util.ipynb, ./Makefile~, ./text1.csv, ./TAGS, ./akka.log, ./build.sbt, ./postamble.sc, ./.classpath, ./programming_in_scala_2nd.pdf, ./Notes.txt, ./.gitignore, ./Makefile, ./scala.txt, ./rev-users.csv, ./basic_example.ipynb, ./make.log, ./.cache-main, ./.project, ./spark-joins.ipynb, ./Notes.txt~, ./rev-devices.csv, ./cscope.out, ./.emacs.desktop, ./cscope.files, ./LICENSE, ./preamble.sc, ./.cache-tests, ./somefile.txt, ./help.log, ./README.markdown, ./mains.lst)\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.flist(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "## Of no use in a Scala notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets write some scala!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6bed5a7a\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: String = 2.4.4\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@768157d9\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class scala.collection.immutable.$colon$colon\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data: Seq[(String, Int)] = List((a,0), (b,1), (c,2), (d,3))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\"a\", \"b\", \"c\", \"d\") zip (0 to 4)\n",
    "\n",
    "U.printClass(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: Seq[(String, Int)] = List((foo,1), (bar,2), (baz,3))\n",
       "data1: Seq[(String, Int)] = List((foo,4), (bar,5), (bar,6))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\"foo\", \"bar\", \"baz\") zip 1 :: 2 :: 3 :: Nil\n",
    "val data1 = Seq(\"foo\", \"bar\", \"bar\") zip 4 :: 5 :: 6 :: Nil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.sql.Dataset\n",
      "class org.apache.spark.rdd.ParallelCollectionRDD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]\n",
       "ds1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[7] at parallelize at <console>:36\n",
       "ds2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[8] at parallelize at <console>:41\n",
       "res6: Array[(String, (Int, Int))] = Array((bar,(2,6)), (bar,(2,5)), (foo,(1,4)))\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds = spark.createDataset(data)\n",
    "\n",
    "val ds1 = sc.parallelize(data)\n",
    "\n",
    "U.printClass(ds)\n",
    "U.printClass(ds1)\n",
    "\n",
    "val ds2 = sc.parallelize(data1)\n",
    "\n",
    "ds1.join(ds2).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "local2: String => java.net.URI = <function1>\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Local file URI\n",
    "// non-existent file loads\n",
    "// /misc/build/0/prog-scala-2nd-ed-code-examples\n",
    "val local2 = U.local1(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.rdd.MapPartitionsRDD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "f1: String = rev-users.csv\n",
       "file: org.apache.spark.rdd.RDD[String] = file:/misc/build/0/prog-scala-2nd-ed-code-examples/rev-users.csv MapPartitionsRDD[19] at textFile at <console>:33\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val f1 = \"rev-users.csv\"\n",
    "val file = sc.textFile(local2(f1).toString())\n",
    "U.printClass(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h2: Seq[String] = WrappedArray(user_id, birth_year, country, city, created_date, user_settings_crypto_unlocked, plan, attributes_notifications_marketing_push, attributes_notifications_marketing_email, num_contacts, num_referrals, num_successful_referrals)\n",
       "r1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[83] at subtract at <console>:35\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This file has a header row\n",
    "// Take the first row, index into it, split and return a sequence\n",
    "val h2 = file.take(1)(0).split(\",\").toSeq\n",
    "\n",
    "// Get the remainder by using subtract\n",
    "// convert the header row back to an RDD using parallelize\n",
    "val r1 = file.subtract(sc.parallelize(file.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: Array[String] = Array(user_1113,1954,GB,Billericay,2018-01-26 07:34:13.040468,1,PREMIUM,,,2,0,0)\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Look at the underlying row\n",
    "r1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df0: org.apache.spark.sql.DataFrame = [user_id: string, birth_year: int ... 10 more fields]\n",
       "res34: Array[org.apache.spark.sql.Row] = Array([user_1113,1954,GB,Billericay,2018-01-26 07:34:13.040468,1,PREMIUM,,,2,0,0])\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Now map over the quantities\n",
    "// The transformations are only applied when we take(), use the column names from h2.\n",
    "val df0 = r1.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b.toInt,c,d,e,f.toInt,g,h,i,j.toInt,k.toInt,l.toInt)}.toDF(h2:_*)\n",
    "df0.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.rdd.MapPartitionsRDD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "f2: String = rev-devicees.csv\n",
       "file2: org.apache.spark.rdd.RDD[String] = file:/misc/build/0/prog-scala-2nd-ed-code-examples/rev-devicees.csv MapPartitionsRDD[107] at textFile at <console>:30\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val f2 = \"rev-devicees.csv\"\n",
    "val file2 = sc.textFile(local2(f2).toString())\n",
    "U.printClass(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lens: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[20] at map at <console>:29\n",
       "res11: Array[Int] = Array(212, 73, 68, 74, 72)\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// But error results here if file does not exist\n",
    "// Or returns empty array if it is empty\n",
    "val lens = file.map(s => s.length)\n",
    "file.take(5)\n",
    "lens.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0: Array[String] = Array(user_id,birth_year,country,city,created_date,user_settings_crypto_unlocked,plan,attributes_notifications_marketing_push,attributes_notifications_marketing_email,num_contacts,num_referrals,num_successful_referrals)\n",
       "pairs: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[21] at map at <console>:32\n",
       "counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[22] at reduceByKey at <console>:33\n",
       "counts1: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[26] at repartition at <console>:35\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val x0 = file.take(1)\n",
    "\n",
    "// Some arbitrary file processing - append a number to each line\n",
    "val pairs = file.map(s => (s, 911))\n",
    "val counts = pairs.reduceByKey((a, b) => a + b)\n",
    "\n",
    "val counts1 = counts.repartition(1)\n",
    "\n",
    "U.rmdir(\"counts1\")\n",
    "counts1.saveAsTextFile(local2(\"counts1\").toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairs: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[28] at map at <console>:29\n",
       "pairs1: org.apache.spark.rdd.RDD[(String, (String, String))] = MapPartitionsRDD[31] at join at <console>:31\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pairs = file.map(x => (x.split(\",\")(0), x))\n",
    "\n",
    "val pairs1 = pairs.join(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id,birth_year,country,city,created_date,user_settings_crypto_unlocked,plan,attributes_notifications_marketing_push,attributes_notifications_marketing_email,num_contacts,num_referrals,num_successful_referrals\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "x1: Seq[String] = WrappedArray(user_id, birth_year, country, city, created_date, user_settings_crypto_unlocked, plan, attributes_notifications_marketing_push, attributes_notifications_marketing_email, num_contacts, num_referrals, num_successful_referrals)\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Make some (K, V) tuples\n",
    "\n",
    "println(x0(0))\n",
    "\n",
    "val x1 = x0(0).split(\",\").toSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileToDf: org.apache.spark.sql.DataFrame = [user_id: string, birth_year: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df0 = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b,c,d,e,f,g,h,i,j,k,l)}.toDF(x1:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fileToDf: org.apache.spark.sql.DataFrame = [user_id: string, birth_year: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// The x1:_* is to be preferred to this\n",
    "\n",
    "// val fileToDf = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "// (a,b,c,d,e,f,g,h,i,j,k,l)}.toDF(\"user_id\", \"birth_year\", \"country\", \"city\", \"created_date\", \"user_settings_crypto_unlocked\", \"plan\", \"attributes_notifications_marketing_push\", \"attributes_notifications_marketing_email\", \"num_contacts\", \"num_referrals\", \"num_successful_referrals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df0: org.apache.spark.sql.DataFrame = [user_id: string, birth_year: int ... 10 more fields]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df0 = file.map(_.split(\",\")).map{case Array(a,b,c,d,e,f,g,h,i,j,k,l) => \n",
    "(a,b.toInt,c,d,e,f,g,h,i,j,k,l)}.toDF(x1:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+------+--------------------+-----------------------------+--------+---------------------------------------+----------------------------------------+------------+-------------+------------------------+\n",
      "|user_id|birth_year|country|  city|        created_date|user_settings_crypto_unlocked|    plan|attributes_notifications_marketing_push|attributes_notifications_marketing_email|num_contacts|num_referrals|num_successful_referrals|\n",
      "+-------+----------+-------+------+--------------------+-----------------------------+--------+---------------------------------------+----------------------------------------+------------+-------------+------------------------+\n",
      "|user_id|birth_year|country|  city|        created_date|         user_settings_cry...|    plan|                   attributes_notifi...|                    attributes_notifi...|num_contacts|num_referrals|    num_successful_re...|\n",
      "| user_0|      1989|     PL|Gdansk|2018-01-13 05:15:...|                            1|STANDARD|                                    1.0|                                     1.0|           3|            0|                       0|\n",
      "| user_1|      1975|     GB|London|2018-01-29 03:38:...|                            0|STANDARD|                                       |                                        |          21|            0|                       0|\n",
      "+-------+----------+-------+------+--------------------+-----------------------------+--------+---------------------------------------+----------------------------------------+------------+-------------+------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fileToDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: Array[Array[String]] = Array(Array(user_id, birth_year, country, city, created_date, user_settings_crypto_unlocked, plan, attributes_notifications_marketing_push, attributes_notifications_marketing_email, num_contacts, num_referrals, num_successful_referrals))\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.map(_.split(\",\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[78] at subtract at <console>:29\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = file.subtract(sc.parallelize(file.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.SparkContext\n"
     ]
    }
   ],
   "source": [
    "U.printClass(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: Array[String] = Array(user_1113,1954,GB,Billericay,2018-01-26 07:34:13.040468,1,PREMIUM,,,2,0,0)\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split: (f1: String, sep: String)(implicit sc: org.apache.spark.SparkContext)org.apache.spark.rdd.RDD[String]\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split(f1:String, sep:String)(implicit sc: org.apache.spark.SparkContext) : org.apache.spark.rdd.RDD[String] = {\n",
    "    val f = sc.textFile(f1)\n",
    "    return f\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: org.apache.spark.rdd.RDD[String] = file:/misc/build/0/prog-scala-2nd-ed-code-examples/rev-users.csv MapPartitionsRDD[109] at textFile at <console>:30\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(local2(f1).toString(), \",\")(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.SparkContext\n"
     ]
    }
   ],
   "source": [
    "U.printClass(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
